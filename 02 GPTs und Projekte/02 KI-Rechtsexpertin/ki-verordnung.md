Erste Hilfe zur
KI-Verordnung

KI-Kompetenz Rechte Pflichten

C.H.BECK

Inhaltsverzeichnis

K I - O u i c k s t a r t

1.  W a s  ist KI?

2.  W a s  ist nicht KI?

5

6

6

3.  W o r a u f  basiert KI und was hat es  m i t  KI-Modellen und KI-Systemen auf sich?

6

4.  Wa s  ist KI mit allgemeinem Verwendungszweck (GPAI)?
5.  W i e   f u n k t i o n i e r t  KI?

7

6

6.  W a r u m  gibt es besonderes KI-Recht (KI-VO)?
7.  F ü r  wen gilt das KI-Recht?

7

8.  Wa s  bedeutet es, ein KI-System zu betreiben?

9.  M u s s  jeder KI-Kompetenz nachweisen?
10. Was bedeutet KI-Kompetenz konkret?

11. Was ist der rechtliche Ansatz der KI-VO?
12. Ist KI gefährlich?

9

7

8

8
8

9

9

13. Wie  f u n k t i o n i e r t  der risikobasierte Ansatz der KI-VO?
14. Wann ist KI nicht riskant?
15.  Wa n n  ist KI hochriskant?
16. Gibt es Ausnahmen von der Einordnung eines KI-Systems als hochriskant?
17.  Wa n n  darf man hochriskante KI konkret verwenden?
1
18. Woran erkennt man hochriskante Anwendungen?
19. Wann ist KI verboten?
1
1
20. Wer haftet, wenn etwas bei der Verwendung von KI schiefgeht?
21. Was müssen Arbeitnehmer und Arbeitgeber bei der Verwendung von KI
1

beachten?

9
9

1

1

1

1

2

22. Ab wann  gilt das KI-Recht?
23. Welches Recht muss man neben dem KI-Recht beachten?1

1

3

4

W a s   i s t   K I ?

1

5

1. A u t o n o m e  Systeme1
2.  R i s i k e n  des KI-Einsatzes1

W i e   k a n n   K I   e i n g e s e t z t   w e r d e n ? 1

1.  E i n s a t z  zu nützlichen Zwecken1
2. Einsatz zu schädlichen Zwecken2

6

7

8

9
0

W e l c h e   P f l i c h t e n   h a t   d e r   B e t r e i b e r   n a c h   d e r   K I - V O ? 2

1

2

3

4

1.  Ve r b o t e n e  Zwecke
2.  H o c h r i s k a n t e  Zwecke
3. Betreiberpflichten   f ü r  Hochrisiko-KI-Systeme2

2

2

2

2

4

4. Transparenzpflichten
5. We ch s el  der Pflichten: Vom Betreiber zum Anbieter per Zweckänderung

2

5

6. KI-Kompetenz
7. Bestandsschutz für „alte" KI
8. Schnellcheck: Geltung der Betreiberpflichten  i m  Einzelfall

2

2

8

8

2

1

0

2

2

6

9

I n h a l t s v e r z e i c h n i s  I  3

5

KI und Datenschutz3
1. Anwendbarkeit des Datenschutzrechts
2. Ausnahme für private Nutzung
3. Rechtsgrundlagen
4. Dokumentationspflichten
5. Allgemeine Informationspflicht
6. Drittstaatentransfer

3

3

3

3
KI und Arbeitsrecht
3
1.  K I  in der Arbeitswelt
2. KI-Verbote: Was Arbeitgeber nicht dürfen
3. Hochriskante KI am Arbeitsplatz
4. „Normales" Arbeitsrecht

3

KI und Verbraucherschutzrecht
1. Verbraucherrechte
2. Recht auf Beschwerde
3. Recht auf Erläuterung
4. Whistleblowing
5. Transparenz

4

4

4

4
4

3

3

3

0
3

1

2

4

2

4

5

6
7

3

7

8

8

4

0

1
1
1

1

1

KI und Urheberrecht
1. Schutz der Eingabe
2. Schutz der Ausgabe
3.  W i e  schütze ich meine Werke vor einer Verwertung im KI-System?

4
4

3
3

4

2

4

5

Glossar

4

6

KI-Checkliste für Betreiber
Checkliste Prompts

4

4

7

8

6

7

8

41 Inhait§verzeichtus

1 KI-Ouickstart

Am 1. August 2024 ist die KI-Verordnung (KI-VO) in Kraft getreten. Sie setzt
einen für die EU einheitlichen Rechtsrahmen für die Verwendung von
KL Wer die neue Technik verwendet, muss zentrale Fragen beantworten
können und wissen, was das neue Recht voraussetzt, um die neue Rechts-
pflicht der KI-Kompetenz zu erfüllen. Dieses Kapitel dient dazu, Fragen
aufzuwerfen und kurz zu beantworten. Damit soll ein Bewusstsein für die
Probleme und deren Lösungen entstehen. Im Rahmen der folgenden Kapi-
tel werden die Probleme aufgegriffen und genauer beantwortet.

-

1. Was ist KI?

Nach der Definition der KI-VO (Verordnung über Künst-
liche Intelligenz) ist KI eine besondere, insbesondere
autonome und deshalb unbeherrschbare Technik, die
sich ohne menschliches Zutun verändern kann. Man
kann eine KI mit einem Tier vergleichen, dessen Wesen
man nicht beherrschen kann. Allerdings kann man
seine Verwendung verantworten, wenn man es verant-
wortlich einsetzt.

Vor allem muss man sich darüber klar sein, dass eine KI
niemals Verantwortung tragen kann und im Rechtssinn
auch keine Fehler begehen kann. Schuld ist immer der
Mensch (siehe 2.1 Autonome Systeme).

2. Was ist nicht KI?

In vielen Fällen ist die KI-VO gar nicht einschlägig, weil
es nicht um KI im Sinne der KI-Verordnung geht. Soge-
nannte Expertensysteme gleichen nur Muster ab, ohne
für autonomen Betrieb angelegt und anpassungsfähig
zu sein. Was sich nicht selbst verändern kann, unter-
fällt dem KI-Recht erst gar nicht (siehe 2.1 Autonome
Systeme).

g4> BEISPIEL

Setzt man solche Systeme im Unternehmen zur Aus-
wertung von Verträgen ein, dann kann man ihnen
Aufgaben nach festen Vorgaben stellen. Welcher
Vertrag ist von Verjährung betroffen? Finden sich in
Firmenunterlagen Hinweise darauf, ob eine Umstel-
lung von Währungen in Verträgen bei der Zahlung
Wechselkursgewinne ermöglicht? Wie entwickelt
sich Kaufverhalten, so dass man beim Warenbestand
besser disponieren kann? Ist eine Darmunebenheit
oder eine Hautveränderung gutartig und wo muss
man als Arzt genau hinsehen? Hier gelten etwa
die Vorgaben der Datenschutz-Grundverordnung
(DSGVO) für den maßgeblichen menschlichen Bei-
trag bei automatisierten Einzelentscheidungen, aber
die KI-VO interessiert sich dafür nicht.

3. Worauf basiert KI und was
hat es mit KI-Modellen und
KI-Systemen auf sich?

Die KI-VO gilt für KI-Modelle und KI-Systeme.

KI-Modelle sind große Datenpools. Aus diesen Pools
leiten KI-Systeme Inhalte ab, seien es Texte, Töne oder
Bilder. Die KI-Modelle sind wie Wassertanks, die aus
sehr vielen Quellen gespeist werden, über die die An-
bieter der Modelle bestimmen. Modellanbieter erzeugen
(trainieren) Datenpools mit enorm vielen Informa-
tionen, deren Herkunft und Auswahl so vielfältig wie
bedeutsam und manipulierbar ist. Daten aus China er-
zeugen andere Werte als solche aus Europa oder den USA.

KI-Systeme sind wie Leitungen, die man an den Tank
anschließt, etwa um eine Wasseraufbereitungsanlage
herzustellen. KI-Systeme bauen damit für einen spezi-
fischen Verwendungszweck auf dem Modell auf. Der
Entwickler und Hersteller des Systems, in der KI-VO-
Sprache der „Anbieter" - legt damit den Verwendungs-
zweck eines KI-Systems fest. Um im Bild zu bleiben:
Der Anbieter hat eine Anlage zur Trinkwasseraufbe-
reitung hergestellt und installiert. Der Betreiber kann
nun bestimmen, wann und wie viel Wasser aufbereitet
wird, aber er kann die Anlage nicht eigenständig in ein
System zur industriellen Kühlwasseraufbereitung um-
funktionieren. Der Betreiber  s e i  es ein Unternehmen,
ein Angestellter, eine Lehrkraft oder ein Schüler -
nutzt das System nur innerhalb dieses vordefinierten
Rahmens. Ändert er diesen Rahmen und setzt er das
KI-System zu einem neuen Zweck ein - eben der Kühl-
wasseraufbereitung -, wird er nach der KI-VO als An-
bieter behandelt. Im Normalfall trägt aber der Anbieter
die Verantwortung für die Sicherheit des KI-Systems
während der Betreiber für den korrekten Einsatz im
Rahmen der vorgegebenen Nutzung verantwortlich ist.

4. Was ist KI mit allgemeinem
Verwendungszweck (GPAI)?

KI-Systeme mit allgemeinem Verwendungszweck (GP»
Systeme) haben im Gegensatz zu regulären KI-Systemen
keinen spezifischen Verwendungszweck. Man kann

6   I   K I - Q u I c k b t d r t

sie  w i e  ChatGPT— für beliebige Zwecke nutzen. Der Bot
kann Liebes- und Hassgedichte schreiben lassen. Was
macht eine Software wie ChatGPT technisch so beson-
ders? Obwohl KI Software ist, behandelt man sie recht-
lich wie ein Produkt.

Man kann sich GPAI als Knetmasse vorstellen, aus der
man je nachdem, was man möchte, unterschiedlich ge-
fährliche Produkte formen kann. Aus derselben „digitalen
Knetmasse", kann man sowohl eine Wasserpistole als
auch eine echte Pistole formen. Vielleicht hilft auch das
Bild des 3D-Druckers. Man kann damit eine Tasse oder
eine einsatzfähige Pistole oder ein Messer drucken. Span-
nend wird es bei KI-Systemen wie auch bei Produkten bei
der Entscheidung über die konkrete Verwendung.

Auch die KI-VO macht die Verantwortung für die Ver-
wendung der Knetmasse vom konkreten Verwendungs-
zweck abhängig und weist die Verantwortung faktisch
jedem zu, der KI einsetzt.

Weil GPAIS alles kann und der Unterschied zwischen
Gut und Böse oder Gleich- und Ungleichbehandlung
am Ende nicht rechtskonform programmiert werden
kann, trägt — von Ausnahmen abgesehen — die alleinige
Verantwortung für die Zwecke der Nutzung deren Ver-
wender (Betreiber) (siehe 4.5 Wechsel der Pflichten: Vom
Betreiber zum Anbieter per Zweckänderung).

5. Wie funktioniert KI?

KI basiert auf Computeranwendungen, die große Daten-
pools verarbeiten, um für Menschen gut verständliche
Ergebnisse zu erzeugen. Im Fokus stehen derzeit An-
wendungen, die wie ChatGPT Inhalte in menschlicher
Sprache oder wie Dall-E Bilder hervorbringen, die von
menschlich erzeugten Inhalten gar nicht oder nur
schwer zu unterscheiden sind. Anders als Menschen
„sehen" Maschinen aber nicht mit Augen und sind nicht
„kreativ" wie Menschen. Stattdessen nutzen sie fort-
schrittliche statistische Methoden, um Muster in ihren
Trainingsdaten zu erkennen und die wahrscheinlichsten
Wort- oder Bildfolgen zu generieren. Was oft als KI-„Krea-
tivität" bezeichnet wird, ist in Wahrheit eine hochentwi-
ckelte Form der Mustererkennung und -reproduktion.
KI-Systeme „verstehen" nicht wirklich die Inhalte, die

sie produzieren; ein KI-System weiß nicht, was es sagt
oder zeichnet. Stattdessen wählt es aufgrund bekannter
Muster die Wörter oder Bildteile aus, die am ehesten zu-
sammenpassen könnten. Es ist, als würde man ein Puzzle
zusammensetzen, weil die Teile zusammenpassen, ohne
die Bedeutung des daraus entstehenden Bildes zu ver-
stehen. Obwohl die Ergebnisse oft erstaunlich gut sind,
hat die KI kein echtes Verständnis oder eigene Gedanken
wie ein Mensch. Sie kann nur das wiedergeben und neu
mischen, was sie von Menschen gelernt hat. Dennoch
können KI-Systeme Zusammenhänge in Datensätzen
erkennen, die ein einzelner Mensch nicht sieht, weil sie
mehr Muster gelernt haben (siehe 2.2 Risiken des KI-Ein-
satzes).

6. Warum gibt es besonderes
KI-Recht (KI-VO)?

In Unternehmen und Behörden haben sich KI-Systeme,
die Texte und andere Inhalte generieren, teilweise be-
reits durchgesetzt. Experten sehen sowohl Chancen für
bedeutende Fortschritte als auch Risiken für (unbeab-
sichtigte) schädliche Folgen für die Gesundheit, Sicher-
heit und Grundrechte von Bürgern. Die Verordnung
über künstliche Intelligenz soll Fortschritt im Rahmen
des rechtlich Zulässigen ermöglichen, nicht verhindern.
Das ist wichtig, denn die Menschen in Europa sind
ebenso wie die Wirtschaft auf Fortschritt angewiesen.
Deshalb hat man in der Europäischen Union die KI-Ver-
ordnung verabschiedet.

7. Für wen gilt das KI-Recht?

Die KI-VO gilt faktisch für jedermann. Sie regelt näm-
lich von der Entwicklung bis hin zum Betrieb, sprich die
Verwendung, von KI-Systemen wie ChatGPT die gesamte
Wertschöpfungskette. Auch für Betreiber legt sie dabei
Pflichten fest.

BEISPIEL

Wer also als Handwerker seine Mitarbeiter oder
Kunden per ChatGPT anspricht oder sich einen Wer-
beflyer von einer Bild-KI erzeugen lässt, ist Betreiber,

KI-Quickstart I  7

denn er verwendet ein KI-System in eigener Ver-
antwortung für die berufliche und nicht private
Tätigkeit.

Ebenso sind Lehrer, die ihren Schülern die Ver-
wendung von ChatGPT zur Unterstützung bei den
Hausaufgaben zur Verfügung stellen, Betreiber von
KI-Systemen. Schließlich sind Hausaufgaben keine
Privatsache, sondern werden im Rahmen der staat-
lichen Schulpflicht gemacht.

Insgesamt sind Schulen, die den Einsatz von KI für
Schüler und Lehrer gestatten, oder Unternehmer, die
ihren Mitarbeitern die Verwendung von KI gestatten,
Betreiber. Auch Beschäftigte, die KI ohne Wissen oder
Erlaubnis des Unternehmens einsetzen (zum Beispiel
Übersetzungssoftware), verwenden KI und unterfallen
als Betreiber dem neuen Recht, denn die Verwendung
erfolgt ja zu beruflichen Zwecken (siehe 4. Welche
Pflichten hat der Betreiber nach der KI-VO?).

8. Was bedeutet es, ein KI-System
zu betreiben?

Wer als Bäcker einen Ofen verwendet, der betreibt ihn
im Rechtssinne. Wer ein KI-System wie ChatGPT ver-
wendet, der betreibt es dementsprechend. KI-Systeme
sind keine Spielzeuge, sondern mächtige technische
Instrumente. Da der Gesetzgeber KI-Systeme je nach
deren Verwendungszweck für gefährlicher hält als
Rasenmäher, verlangt er von jedermann Kompetenz-
nachweise für die Verwendung. Auch wenn es, was
unrealistisch wäre, keine Führerscheinpflicht für den
Einsatz von KI gibt, muss man sich dennoch um Kompe-
tenz bemühen. Das ist erforderlich, damit man Erlaubtes
von Verbotenem unterscheiden kann und es möglichst
nicht zu Rechtsverstößen kommt. Die KI-VO knüpft
dementsprechend die Verwendung von KI außerhalb
des privaten Bereichs an die Rechtspflicht zur Vermitt-
lung von KI-Kompetenz (siehe 4. Welche Pflichten hat
der Betreiber nach der KI-VO?).

9. Muss jeder KI-Kompetenz
nachweisen?

Die KI-VO schreibt dazu jedem, der ein KI-System wie
ChatGPT außerhalb des privaten Bereichs verwendet
also betreibt, die Vermittlung von KI-Kompetenz  •
(Artikel 4 KI-VO) vor. Da diese Pflicht für Betreiber, also
Verwender von KI-Systemen gilt, sind nicht nur Unter-
nehmen und Behörden verpflichtet, sondern auch jede
natürliche Person, die ein KI-System wie ChatGPT nicht
zu persönlichen Zwecken nutzt. Diese Pflicht muss im
Februar 2025 umgesetzt sein. Wer diese Broschüre ge-
lesen und verstanden hat, ist einen wichtigen und recht-
lich nötigen Schritt gegangen.

Im Rahmen einer allgemeinen Pflicht muss jeder An-
bieter und Betreiber von KI-Systemen, also faktisch
jeder der Systeme wie ChatGPT in seinem Geschäfts-
kreis einsetzt, sicherstellen, dass seinem Personal und
anderen Personen, die in seinem Auftrag KI-Systeme
nutzen und betreiben etwa durch Schulungen KI-Kom-
petenz vermittelt wird. Das muss nach besten Kräften
erfolgen, darf also nicht nachlässig geschehen (siehe
4.6 KI-Kompetenz).

lo. Was bedeutet KI-Kompetenz
konkret?

Konkrete Fragen die jeder beim Umgang mit KI-Sys-
temen wie ChatGPT beantworten können muss, lauten
etwa wie folgt (siehe auch 4.6 KI-Kompetenz):
•  Wa s  ist ein KI-System, was ist ein KI-Modell, was ist

der Unterschied?

•  Wa s  bedeutet Autonomie von KI?
•  Wa r u m  kann KI nicht denken und trotzdem mit mir

sprechen?

• Welche Nutzung von KI-Systemen ist gefahrlos mög-

lich? Wo muss ich aufpassen?

•  Wa s  bedeutet „prompten" und wie geht das?
•  W i e  setze ich mich mit KI-Ergebnissen auseinander?
•  W i e  behalte ich als Mensch die Kontrolle über das

Werkzeug KI?

•  Wa s  bedeutet der Einsatz von KI im beruflichen

Alltag? Wo kann mir die Technik helfen, wo nicht?

8 1 KI-Qinckstart

11. Was ist der rechtliche Ansatz  1 3 .  Wie funktioniert der risiko-
der KI-VO?

basierte Ansatz der KI-VO?

Die KI-VO wählt einen rechtlichen Ansatz, der aus zwei
Kernelementen besteht. Sie steckt zunächst einen ge-
setzlichen Rahmen für die Entwicklung und den Betrieb
künstlicher Intelligenz ab und ordnet die Nutzung der
Technik in Risikoklassen ein. Sodann löst die KI-VO das
Problem der Sicherung der menschlichen Verantwor-
tung bei maschineller Hilfe, indem sie den Menschen
in die Pflicht nimmt, die autonome Technik selbst-
bestimmt zu stoppen, wenn es sein muss. Jenseits der
Grenzen dieses Rechtsrahmens zum Schutz der Men-
schen und ihrer Rechte herrscht Freiheit zum Einsatz
von KI, soweit nicht das von der KI-VO unberührte und
unabhängig davon geltende sonstige Recht - etwa das
Datenschutz- oder Urheberrecht - ohnehin Grenzen
setzt (siehe 4. Welche Pflichten hat der Betreiber nach
der KI-VO?)

12. Ist KI gefährlich?

Das kommt es auf den konkreten Verwendungszweck
an. Von diesem macht die KI-VO auch die rechtlichen
Grenzen des Einsatzes abhängig und den bestimmt der-
jenige, der die KI verwendet (siehe 4. Welche Pflichten
hat der Betreiber nach der KI-V(3?).

BEISPIEL

Stellen Sie sich ein KI-Übersetzungsprogramm vor,
das wie ein digitaler Dolmetscher funktioniert.
Wenn Sie es im Urlaub benutzen, um ein Restaurant-
Menü zu übersetzen, ist das nicht riskant und des-
halb ohne rechtliche Konsequenzen erlaubt. Nutzt
ein Gericht es für für die Vernehmung mit fremd-
sprachigen Zeugen, muss sichergestellt werden, dass
es sehr genau arbeitet und von Menschen über-
prüft wird. Würde aber jemand dieses Programm so
umbauen, dass es bei der Übersetzung von Miet-
verträgen für fremdsprachige Mieter wichtige Infor-
mationen auslässt oder verändert, um Klauseln zum
Mieterschutz zu unterschlagen, wäre dies verboten.

Die KI-VO stuft das Risiko in Kategorien ein. Sie lauten
wie in einer Pyramide erstens risikolos und erlaubt,
zweitens hochriskant und nur unter strengen Vorausset-
zungen zulässig und drittens verboten. Ist der Einsatz-
zweck hochriskant, gelten sehr strenge und spezifische
Pflichten für den Betrieb eines KI-Systems (siehe 4.
Welche Pflichten hat der Betreiber nach der KI-VO?)

14. Wann ist KI nicht riskant?

Ob eine KI riskant ist oder nicht, hängt von ihrem Ein-
satzzweck ab - egal ob es sich um ein spezialisiertes
System oder einen vielseitigen digitalen Assistenten
(GPAI) handelt. Für alltägliche Aufgaben wie Fotosor-
tierer oder Einkaufslisten-Ersteller gelten keine beson-
deren Regeln. Wird die KI jedoch in sensiblen Bereichen
wie Bildung oder Personalwesen eingesetzt, stuft man
sie als hochriskant ein und reguliert sie strenger. Als un-
bedenklich gelten KI-Systeme, die lediglich Routineauf-
gaben erledigen, menschliche Arbeit unterstützen oder
Muster erkennen, ohne eigenständig wichtige Entschei-
dungen zu treffen. Sobald die KI aber persönliche Profile
erstellt, gilt sie automatisch als hochriskant.

BEISPIEL

Ein Bäcker nutzt eine KI-App „Bäcker-Berater" für Ge-
schäftsideen. Die App analysiert Verkaufszahlen und
Trends, verarbeitet die Vorschläge des Bäckers und
empfiehlt zum Beispiel eine giutenfreie Produkt-
linie oder Mittagssnacks. Das KI-System unterstützt
nur bei der Ideenfindung, trifft aber keine Entschei-
dungen.

15. Wann ist KI hochriskant?

Wann KI hochriskant ist, bestimmt das Recht selbst
und benennt dafür konkrete Bereiche. So ist KI, die die
Bedingungen von Arbeitsverhältnissen beeinflussen

KI-Quickstart I  9

kann oder die für die Bewertung von Lernergebnissen
im Bildungsbereich also bei Schülern, Auszubildenden
oder Studierenden verwendet wird, hochriskant. Andere
Bereiche sind Gesundheit und Justiz.

reguliert agiert, richtet sich nach einer Entscheidung
des Gesetzgebers im Anhang zur KI-VO. Alles, was
Auswirkungen auf die Bedingungen von Arbeitsver-
hältnissen hat, ist nach dem Gesetz hochriskant.

Da man GPAI für allgemeine und beliebige Zwecke ver-
wenden kann, verlangt der Einsatz dieser KI jedermann,
der sie verwendet, eine schwierige Entscheidung ab. Er
muss bei der Verwendung bewerten, ob sie im kon-
kreten Fall hochriskant ist. Das hängt allein vom Zweck
der Verwendung ab (siehe 4.2 Hochriskante Zwecke).

BEISPIEL

Wie kann man GPAI im Beschäftigtenkontext ein-
setzen? Indem man sie a) für die Erstellung des
Speiseplanes für das persönliche Mittagessen im
Büro nutzt. Das ist ein privater Zweck. Man kann den
Bot b) die Kolorierung einer Präsentation des Speise-
plans für gesundes Kantinenessen erstellen lassen.
Das ist dienstlich, hat aber keinen Einfluss auf die
Bedingungen von Arbeitsverhältnissen. Wenn
man die KI aber c) den Speiseplan des kommenden
Monats für gesundes Essen für alle Mitarbeiter —
mit der Maßgabe entweder vegetarisch oder mit
Fleisch — erstellen lässt, dann hat das Bedeutung für
die Bedingungen der Arbeitsverhältnisse. Je nach
Entscheidung der KI könnte dies weitreichende
Auswirkungen auf die Arbeitsbedingungen haben:
Eine einseitige Bevorzugung vegetarischer oder
fleischhaltiger Gerichte könnte bestimmte Mitarbei-
tergruppen benachteiligen, kulturelle oder reli-
giöse Sensibilitäten verletzen oder gesundheitliche
Aspekte vernachlässigen. Lässt man die KI — eben-
falls c) eine (Vor-)auswahl für Beförderungen oder
Kündigungen treffen, dann hat das auch Bedeutung
für den Job. Man kann die KI auch d) dazu nutzen, die
Qualität des Kantinenessens für Lowperfomer oder
unliebsame Mitarbeiter zu verringern. In der Risiko-
Pyramide der KI-VO kommt a) als private Verwen-
dung nicht vor, b) ist risikolos und ohne Vorgaben
erlaubt, c) ist hochriskant und unter sehr strengen
Voraussetzungen erlaubt und d) ist verboten. In der
Praxis ist die Grenze der Verwendungen zwischen
b) und c) interessant. Ob man als Beschäftigter beim
Einsatz von KI ohne Vorgaben erlaubt oder streng

16. Gibt es Ausnahmen von der
Einordnung eines KI-Systems als
hochriskant?

Das Gesetz enthält aber Ausnahmen von dieser Ein-
stufung als hochriskant. Das ist dann der Fall, wenn die
KI nur unmaßgebliche Hilfsaufgaben übernimmt und
ein zuvor gefundenes menschliches Ergebnis optimiert,
aber nicht wesentlich beeinflusst.

Die Fragen, wo die Grenzen des Hilfseinsatzes der KI
liegen, hängen vom Zweck der Verwendung ab (siehe
4.2 Hochriskante Zwecke):
• Gesundheit: Wo verlaufen bei der Diagnose bis hin
zur Entscheidung über Leben und Tod (Triage) die
Grenzen für den faktisch autonom agierenden KI-
Arzt?

• Unternehmen: Wie weit darf der Rat des Kollegen

Chatbot gehen, wenn es um Personalentscheidungen
im Betrieb geht?

• Gericht: Darf ein Bot am Ende dem Richter helfen
und gar Tipps für faire Gerichtsurteile geben?
• Schule: Was darf der „KI-Lehrer" bei der Benotung

von Schülern?

BEISPIEL

Wenn ein Lehrer sich bei der Bewertung einer
Klassenarbeit von ChatGPT helfen lässt, dann ist
dieser Zweck als hochriskant eingestuft. Das könnte
in bestimmten Fällen aber zu streng sein. Deshalb
benennt die KI-VO abschließend vier Fälle, in denen
die Verwendung der KI im Kontext der Bewertung
von Schülern nicht hochriskant sein soll. Erstens:
Lässt der Lehrer im Rahmen der Bewertung nur
eng gefasste Verwaltungsaufgaben erledigen, etwa
die Schüler in alphabetischer Reihenfolge oder
nach zuvor vergebener Note sortieren, greift die

1 0  I KI-Quickstar,

Ausnahme. Zweitens: Hat der Lehrer bereits eine
Note vergeben und begründet, dann kann er per KI
Impulse für das Überdenken seiner Bewertung ein-
holen. Drittens: Hat der Lehrer die Noten vergeben
und begründet und möchte er danach wissen, ob
es Muster oder Abweichungen von Mustern bei der
Notenvergabe gab, kann er diese per KI ermitteln
lassen. Die Note darf das aber nicht beeinflussen.
Viertens: Der Lehrer will nur eine „vorbereitende
Aufgabe" für eine Benotung vornehmen lassen. Da
damit aber Vorbereitungsmaßnahmen wie Indexie-
rung, Suche sowie Text- und Sprachverarbeitung
gemeint sind, lässt diese Ausnahme keine Vorbewer-
tungen oder Bewertungsentwürfe von Schularbeiten
zu. Bei näherem Hinsehen zeigt sich, dass die Aus-
nahmen keine Zweckbestimmungen legitimieren,
mit denen KI-Systemen maßgebliche Aufgaben
übertragen werden können. Insofern ist ihr Nutzen
fragwürdig.

17. Wann darf man hochriskante
KI konkret verwenden?

Wenn man KI verwendet, die als hochriskant eingestuft
ist, dann muss man die strengen Pflichten einhalten,
die die KI-VO daran knüpft. Diese bestehen etwa darin,
passende Eingabedaten auszuwählen, den Betrieb des
KI-Systems zu überwachen, von dem System erzeugte
Protokolle aufzubewahren und von der Verwendung
des Systems betroffene Arbeitnehmer zu informieren.
Zudem muss eine menschliche Aufsicht installiert
werden. Behörden müssen sich schließlich mit der Frage
auseinandersetzen, wie der Einsatz des KI-Systems
die Grundrechte der betroffenen Personen beein-
flusst (siehe 4.3 Betreiberpflichten für Hochrisiko-KI-
Systeme).

BEISPIEL

Was bedeutet das? Setzt ein Arbeitnehmer, dem
die Nutzung von ChatGPT am Arbeitsplatz nicht
gestattet ist, dieses KI-System für die oben ge-
schilderten hochriskanten Zwecke der Rubrik c)
(„Kantinenessen vegetarisch oder mit Fleisch" oder

„Hinweise zur Beförderung oder Kündigung") ein,
dann verändert er durch die Zweckänderung die
Risikoklasse von harmlos in hochriskant. Deshalb
muss sich der Beschäftigte anstelle des Anbieters des
KI-Systems nach den sehr strengen Regeln für den
Anbieter verantworten. So will es die KI-VO.

Hat der Arbeitgeber den Einsatz eines GPAIS zu
dem hochriskanten Zweck gestattet, dann trifft die
Verantwortung diesen. Er wird insoweit anstelle
des Anbieters zum Verantwortlichen. In diesem Fall
treffen die rechtlichen Folgen, sei es wegen eines
Verstoßes nach KI-Recht oder gegen sonstiges Recht,
den Betreiber.

18. Woran erkennt man hoch-
riskante Anwendungen?

Abstrakt klingt das einfach. Zum Schwur kommt es in
konkreten Situationen. Oft kann man hochriskantes
und nicht hochriskantes nur schwer auseinanderhalten
(siehe 4.2 Hochriskante Zwecke).

FAUSTFORMEL

Immer dann, wenn der Einsatz der KI einen Men-
schen in Rechten betreffen kann, also bei der Bewer-
tung in Beruf oder Schule oder bei der Erbringung
öffentlicher Leistungen sollte man zurückhaltend
sein. Sich von der KI eine Geschichte erzählen oder
einen Reisetipp geben zulassen, ist demgegenüber
unproblematisch.

19. Wann ist KI verboten?

Die verbotenen Zwecke legt die KI-VO ebenso fest, wie
die erlaubten und hochriskanten Zwecke. Dazu zählt
unter anderem sogenanntes Social Scoring, bei dem
etwa der Staat seine Bürger per KI manipuliert und
klassifiziert und von dieser Klassifizierung deren staat-
liche Behandlung abhängig macht (siehe 4.1 Verbotene
Zwecke).

K I - Q u I c k s t a r t   I   11

-••••••

cr> BEISPIEL

Eine Recycling-KI bewertet Bürger anhand ihrer
Mülltrennung. Wer schlechter trennt, muss länger
auf Termine im Bürgeramt warten und zahlt höhere
Gebühren für städtische Dienstleistungen.

umfangreiche Bußgeldkataloge mit beträchtlichen
Höchststrafen vor. Im Anwendungsbereich der DS-GV0
haben die Datenschutzaufsichtsbehörden Bußgelder
bisher meist angemessen berechnet. Es darf gehofft
werden, dass auch im Anwendungsbereich der KI-VO die
Höchstbeträge für Ausnahmefälle reserviert bleiben.

20. Wer haftet, wenn etwas
bei der Verwendung von KI
schiefgeht?

Ein Verstoß gegen Pflichten der KI-VO können mit
enormen Bußgeldern in der Spitze in Höhe von vielen
Millionen Euro belegt werden. Da zahlreiche Normen
der KI-VO dem Schutz der Menschen dienen, die von den
Anwendungen betroffen sind, tritt zudem die Haftung
der Betreiber nach dem Schadensersatzrecht des Bürger-
lichen Gesetzbuches ein, wenn sie gegen diese Normen
der KI-VO verstoßen. Da dieselben Handlungen mit
Datenverarbeitungen verbunden sind, dürfte zusätz-
lich auch an die Haftung auf Schadensersatz nach der
DSGVO zu denken sein. Wie gesagt: Wenn Anbieter,
also Hersteller von KI-Modellen und KI-Systemen wie
Open Al bei ChatGPT diese für beliebige und allgemeine
Zwecke anbieten, dann sind sie nicht ohne Weiteres für
konkrete Anwendungen durch Betreiber verantwort-
lich. Zunächst kommt eine Haftung des Anwenders
für Rechtsverletzungen in Betracht. Veröffentlicht der
KI-Anwender etwa KI-generierte Inhalte, die urheber-
rechtlich geschützte Werke enthalten, drohen Schaden-
ersatzforderungen des Rechtsinhabers. Gleiches gilt,
wenn ein KI-System persönlichkeitsrechtsverletzende
Inhalte generiert, die der Anwender veröffentlicht.
Insbesondere im Urheberrecht wird von den Gerichten
ein strenger Maßstab für den Vorwurf der Fahrlässig-
keit angelegt. Betreiber sollten deshalb sicherstellen,
dass ihre Mitarbeitenden darüber aufgeklärt sind, dass
auch KI-generierte Inhalte wie Bilder oder Texte eine
Vervielfältigung eines geschütztes Werkes im Sinne
des Urheberrechts sein können. Daneben kommt eine
Haftung in Betracht, wenn Betreiber zum Beispiel ihre
Transparenzpflichten nach Art. 50 KI-VO nicht erfüllen.
Daneben drohen bei Verstößen gegen die KI-VO und
gegen die DS-GVO Bußgelder. Die beiden Gesetze sehen

BEISPIEL

Wer also als Arbeitgeber ein GPAIS mit Informa-
tionen über seine Beschäftigten füttert, und sich
von der KI Empfehlungen für deren Zusammen-
arbeit und dafür, wen man vielleicht kündigen soll
geben lässt, der nutzt eine KI, die nur für allgemeine
Zwecke angeboten wird, eigenmächtig für hoch-
riskante Zwecke im Beschäftigtenkontext. Dafür
kann der Anbieter des KI-Systems nichts. Auch der
Hersteller von Kühlwasser kann nichts dafür, wenn
Menschen es in Trinkwasserflaschen füllen und es
dann auch trinken. Ebenso kann der Anbieter eines
KI-Systems nichts dafür, wenn man es zu Zwecken
verwendet, für die es nicht gedacht ist. Deshalb
haftet er auch nicht für Ergebnisse, die unter Verstoß
gegen das Urheber-, Marken- oder Datenschutzrecht
erzeugt werden.

—› HINWEIS

Wer dieses Risiko als Betreiber beherrschen will, der
kann dafür sorgen, dass ein KI-System nur auf seinen
Datenpool mit Verträgen, Warenbestand etc. zugreift.
Solche Angebote sind am Markt verfügbar, aber teuer,
weil der Systemanbieter das Risiko übernimmt und
es sich bezahlen lässt.

21. Was müssen Arbeitnehmer
und Arbeitgeber bei der Verwen-
dung von KI beachten?

Die KI-VO gilt unter anderem für jede natürliche oder
juristische Person, die ein KI-System in eigener Verant-
wortung verwendet. Persönliche und nicht berufliche
Tätigkeiten sind ausgenommen. Viele Unternehmen
lassen die Nutzung von KI zu. Sie müssen dafür nach
der KI-VO als Betreiber und nach der DS-GVO als

1 2   I   K I - Q u i c k s t a r l

Verantwortliche geradestehen. Andere Arbeitgeber
gestatten nicht, dass ihre Beschäftigten Denk- und Prüf-
aufgaben an Computer übertragen, die autonom und
insofern für Arbeitgeber und Mitarbeiter letztlich unbe-
herrschbar agieren. Es muss ja nicht jedem geheuer sein,
bei der Arbeit Maschinen zu benutzen, die dem Men-
schen nicht wie Suchmaschinen nach kalkulierbaren
technischen Vorgaben beim Finden von Ergebnissen
helfen, sondern die Lösungen nach nicht beherrsch-
baren Regeln autonom erfinden.

Sich als Arbeitgeber vor dem unautorisierten Einsatz
von KI zu schützen, ist schwierig. Beschäftigte können
sich leicht über ein betriebliches Verbot hinwegsetzen.
Dazu können sie ein privat erworbenes GPAI-System
nutzen. Das geschieht dann allerdings in eigener Verant-
wortung nach der KI-VO auf der einen und sonstigem
Recht, etwa der DS-GVO auf der anderen Seite.

Beschäftigte können sich in diesem Fall nicht aus der
Verantwortung stehlen, weil die private KI im Unter-
nehmen „im Rahmen einer persönlichen und nicht
beruflichen Tätigkeit verwendet" wird. Der Einsatz dient
beruflichen Zwecken des Arbeitgebers. Genauso handelt
ein Beschäftigter nicht privat, wenn er alle dienstlichen
Dokumente mit eigener Software auf einem privaten
Rechner erstellt und sie mit einem privaten Stift unter-
schreibt (siehe 4.5 Wechsel der Pflichten: Vom Betreiber
zum Anbieter per Zweckänderung).

22. Ab wann gilt das KI-Recht?

Die KI-VO ist am 1. August 2024 in Kraft getreten. Damit
sich Staat, Gesellschaft und Wirtschaft an den neuen
Rechtsrahmen gewöhnen können, gelten die Rege-
lungen stufenweise. Die Berechnung aller Geltungs-
fristen beginnt am 2. August 2024. Die wesentlichen
Geltungsschritte für Unternehmen sind:

Bereits ab dem 2. Februar 2025 müssen Anbieter und
Betreiber von KI-Systemen - letzteres ist jeder, der KI im
beruflichen Kontext in eigener Verantwortung nutzt
-  sicherstellen, dass ihr Personal über ausreichende
KI-Kompetenz verfügt. Die KI-VO verlangt damit ein
grundlegendes Verständnis für die Systeme sowie alle
Fähigkeiten und Kenntnisse, die ihren sachkundigen

Einsatz ermöglichen. Wer KI-Systeme verwendet, soll
sich der Chancen von KI, aber auch ihrer Risiken und
möglicher Schäden bei ihrem Einsatz bewusst sein.
Unternehmen und Behörden, die KI-Systeme in ihre
Prozesse integrieren wollen oder sie bereits integriert
haben, sollten deshalb schon jetzt ein Konzept zur Wei-
terbildung ihrer Mitarbeiter entwickeln.

Ebenfalls ab dem 2. Februar 2025 gelten die Verbote für
bestimmte KI-Praktiken. Die Verwendung von KI-Sys-
temen zur unterschwelligen Beeinflussung, zur sozialen
Bewertung oder etwa zur Ableitung von Emotionen ist
ab diesem Tag untersagt. Der Gesetzgeber begründet die
vorgezogene Geltung mit dem unannehmbaren Risiko,
das von diesen Praktiken ausgeht. Wer KI-Systeme
einsetzt, sollte sich so früh wie möglich mit der Frage
auseinandersetzen, ob die konkrete Verwendung einer
der verbotenen Praktiken unterfällt. So ist etwa von
einer verbotenen Ableitung von Emotionen auszugehen,
wenn die Prüfungsangst von Schülern oder die Zufrie-
denheit von Arbeitnehmern durch einen KI-basierten
Chatbot ermittelt wird. Eine falsche Einschätzung führt
zwar zunächst nicht zu staatlichen Sanktionen, da die
Sanktionsvorschriften erst zu einem späteren Zeitpunkt
Geltung beanspruchen. Als gesetzliches Verbot könnten
aber Verträge zur Erstellung, Nutzung oder Vertrieb
einer solchen KI-Anwendung nichtig sein. Sie kann aber
bereits Schadensersatzansprüche auslösen.

Am 2. August 2025 wird sodann der infrastrukturelle
Grundstein für die umfängliche Geltung der KI-VO
gelegt: Ab diesem Tag gelten die Vorschriften der KI-
VO, die die Durchsetzung des Rechtsakts sicherstellen
sollen. Der Staat muss bis zum 2. August 2025 deshalb
eine Leitungsstruktur aufbauen und Verfahren eta-
blieren, sodass er die Einhaltung der KI-VO überwa-
chen kann. Dazu muss er insbesondere die zuständige
Marktüberwachungsbehörde benennen. In Deutschland
deutet derzeit einiges auf die Bundesnetzagentur hin,
die Datenschutzaufsichtsbehörden haben allerdings
ebenfalls ein Interesse an der Übernahme der Aufsicht
angemeldet.

Ein Jahr später, am 2. August 2026, beginnt die Gel-
tung des größten Teils des Rechtsakts. Ab diesem Tag
müssen Hochrisiko-KI-Systeme die besonderen Anfor-
derungen an Transparenz, Datenqualität, Genauigkeit,

KI-QuIckstart  1 1 3

Robustheit und vieles mehr erfüllen. Für bestimmte
KI-Systeme gelten ab diesem Tag zudem besondere
Transparenzpflichten. Insbesondere der Einsatz von
KI-Systemen zur Generierung von Inhalten wie Texten,
Bildern, Videos oder Musik muss dann grundsätzlich
kenntlich gemacht werden. Unternehmen und Be-
hörden, die KI-Systeme einsetzen, sollten diesen Tag rot
im Kalender markieren. Denn eine Vielzahl der dann
geltenden Vorschriften betrifft Hochrisiko-KI-Systeme.
Darunter fallen etwa Systeme, die bestimmungsgemäß
im Bildungssektor oder im Beschäftigungskontext
eingesetzt werden. Werden KI-Systeme zu einem der
genannten Zwecke verwendet, müssen die verantwort-
lichen Betreiber, das heißt die Unternehmen und Be-
hörden, unter anderem einen Einsatz im Einklang mit
der Betriebsanleitung sicherstellen und eine mensch-
liche Aufsicht installieren, die den Betrieb überwacht.
Für Behörden, die KI-Systeme in hochriskanten An-
wendungsbereichen einsetzen, tritt die Pflicht hinzu,
sich über die Auswirkungen des KI-Einsatzes auf die
Grundrechte der betroffenen Personen nachweislich zu
vergewissern (Grundrechte-Folgenabschätzung). Die
Umsetzung dieser Pflichten dürfte einige Organisation
beanspruchen. Vorbereitende Maßnahmen sollten des-
halb schon jetzt ergriffen werden.

Ab dem 2. August 2027 finden die Vorschriften für
Hochrisiko-KI-Systeme auch Anwendung auf KI-Sys-
teme, die als Sicherheitsbauteile spezifisch regulierter
Produkte dienen oder selbst entsprechende Produkte
sind.

Für bestimmte Systeme, die vor dem 2. August 2026 bzw
vor dem 2. August 2027 in Verkehr gebracht Oder in Be-
trieb genommen wurden, gelten besondere Ausnahmen
mit Blick auf die Einhaltung der Vorschriften der KI-VO
Für diese läuft eine letzte Frist am 31. Dezember 2030 ab.

23. Welches Recht muss man
neben dem KI-Recht beachten?

Mit der KI-VO ist es nicht getan. Wer KI-Systeme ver-
wendet, der muss nicht nur die Regeln der KI-VO ein-
halten. Da bei der Verwendung von KI-Systemen Texte
und Bilder entstehen, deren Grundlagen geschützt sind
und auf Inhalte zugegriffen wird, die geschützt sind,
muss man zusätzlich das Urheberrecht und das Mar-
kenrecht beachten. Das gilt bei der Verwendung von
KI ohnehin und ebenso wie das Datenschutzrecht, da
Daten verarbeitet werden. Das Verbraucherschutzrecht,
das Arbeitsrecht und das Jugendschutzrecht gelten
ebenso (siehe 5. KI und Datenschutz, 6. KI und Arbeits-
recht, 7. KI und Verbraucherschutzrecht und 8. KI und
Urheberrecht).

BEISPIEL

Beim Einsatz von KI-Systemen muss man unab-
hängig von der KI-VO das sonstige Recht beachten.
Das ist immer so. Wer zum Beispiel etwas stiehlt,
macht sich strafbar und muss sich nach dem Straf-
gesetzbuch verantworten. Zugleich muss der Dieb
die Sache nach dem Bürgerlichen Gesetzbuch dem
Eigentümer zurückgeben.

1 4  KI-Quickstart

2 Was ist KI?

Diese Broschüre soll eine erste Hilfe zum rechtssicheren Einsatz künstlicher
Intelligenz (KI) in Ihrem Unternehmen bieten. Das wirft die Frage auf was
eigentlich unter KI zu verstehen ist. Unter „künstlich" können wir uns noch
etwas vorstellen, aber wie lässt sich „Intelligenz" definieren? Seit in einem
Forschungsantrag im Jahr 1956 erstmals der Begriff der künstlichen Intelli-
genz erwähnt wurde, hat sich sein Verständnis stetig gewandelt. Erscheint
eine neue Technologie, verliert die KI oft die ihr zugeschriebene Aura des
Außergewöhnlichen; es entsteht der Eindruck, KI sei lediglich das, was
maschinelle Systeme aktuell noch nicht leisten können. Für die rechtliche
Auseinandersetzung mit KI bedarf es einer präziseren Definition. Entschei-
dend ist die Frage, ob es Technologien gibt, die eine besondere rechtliche
Betrachtung erfordern.

1. Autonome Systeme

Immer mehr Computerprogramme verfügen heute über
Autonomie. In diesem Kontext meint Autonomie die Fä-
higkeit eines Programms, eine Handlung ohne mensch-
lichen Eingriff auszuführen. Die relevante Handlung
liegt allerdings nicht in der Erfüllung einer vom Nutzer
gestellten Aufgabe, sondern in der Veränderung der
Handlungsanweisungen, die der menschliche Pro-
grammierer zur Aufgabenerfüllung vorgegeben hat. Das
mag im ersten Zugriff kontraintuitiv erscheinen, trifft
aber im Kern den Grund einer besonderen Regulierung
dieser Technologien.

BEISPIEL: AMPELANLAGE

Denken Sie an eine Ampelanlage, welche den Ver-
kehr ohne menschlichen Eingriff regelt. Die Ampel
wird ihre Aufgabe immer auf die Weise ausführen,
die ihr vorgegeben wurde. Für manche Kreuzungen
und bestimmte Uhrzeiten erzielt sie damit gute
Ergebnisse, ändern sich die Bedingungen, frustriert
sie Verkehrsteilnehmer mit kurzen Grünphasen zu
Stoßzeiten oder Stillstand trotz fehlendem Verkehr.
Das mag für manche Verkehrsteilnehmer ärger-
lich sein, die hinter der Ampelanlage stehende
Technologie ist aber nicht gefährlich — sie führt von
Menschen bestimmte Regeln aus. Eine autonome
Ampelanlage ist dagegen befähigt, ihre Schaltung
ohne menschlichen Eingriff zu verändern und auf
das konkrete Verkehrsaufkommen maßzuschnei-
dern. Sie könnte etwa erkennen, dass die Überque-
rung für Fußgänger mit Kindern länger dauert oder
der Bremsweg eines 16-Tonners ein früheres Rot-
zeichen benötigt, als ein Moped. Der erhoffte tech-
nologische Vorteil dieser Systeme ist die maschinell
durchgeführte Personalisierung von Diensten.

Die Lösungen für manche Probleme kann der Mensch
nicht so in Programmiersprache übersetzen, dass
eine Maschine die Aufgabe nach seinen Vorstellungen
übernehmen kann. Das kann daran liegen, dass er die
Lösung selbst nicht kennt oder aber daran, dass er sie
nicht ausdrücken kann. Für diese Fälle wurden techno-
logische Verfahren entwickelt, in denen die Maschine
ihre eigene Programmierung übernimmt, indem sie die

Handlungsanweisungen des Menschen interpretiert
und optimiert. Dieses Verfahren wird in der Infor-
mationstechnik als maschinelles Lernen bezeichnet
Zuvor benachteiligte Verkehrsteilnehmer könnten  •
sich über eine derart autonome Ampelanlage freuen.
Optimierung ist aber stets eine Frage der Perspektive:
Was für die Maschine eine Verbesserung der Umstände
bedeutet, kann dem Menschen schaden. Das wäre kein
Problem, wenn der Mensch nachverfolgen könnte,
wie das System die vorgegebenen Handlungsanwei-
sungen verändert. Bei einer Fehlentwicklung könnte
er nachbessern, im schlimmsten Fall sprichwörtlich
den Stecker ziehen. Eine Überprüfung des Systems ist
allerdings mit erheblichen Schwierigkeiten verbunden:
Sobald das System einmal begonnen hat, die Hand-
lungsanweisungen zu verändern, ist für den mensch-
lichen Programmierer nicht mehr nachvollziehbar,
was im Maschinenraum vor sich geht. Deshalb werden
autonomen Systeme auch als Blackbox bezeichnet.

BEISPIEL: AUTONOMIE

Der Jäger trainiert seinen Hund monatelang da-
rauf, eine geschossene Ente zu apportieren. Auf der
Jagd gehorcht der Hund zunächst, einige Jahre lang
schafft er brav die Enten heran. Eines Tages aller-
dings bringt er statt der geschossenen Ente eine
Nachbarskatze, die er selbst erlegt hat. Die Hand-
lungsanweisungen des Jägers waren klar: „Bring mir
die geschossene Ente." Irgendwann hat der Jagdhund
diese Anweisungen allerdings verallgemeinert:
„Bring mir die Beute." Dass der Jäger weder Interesse
an einer erlegten Katze noch an einem Nachbar-
schaftsstreit hatte, konnte der Hund nicht wissen.
Dem Jäger wurden die Grenzen der Beherrschbar-
keit seines Tieres aufgezeigt. Gleichwohl trägt er die
Konsequenzen. Die Autonomie der Tiere zeigt sich
nicht nur im Bild des eigenständig jagenden Hundes.
Der Schoßhund macht nicht immer Sitz, wenn er
soll, und der Umgang mit domestizierten Wildtieren
bleibt stets gefährlich.

Geht es um eine Ampelanlage wird das Risiko dieser
Technologie deutlich: Geht ein nicht nachvollziehbares,
autonomes Ampelsystem fehl, kann das lebensgefähr-
lich sein.

1 6  1  Wa s  ist KP

Der europäische Gesetzgeber hat das Gefahrenpoten-
zial dieser Technologie erkannt und die Autonomie als
entscheidendes Merkmal regulierungsbedürftiger
KI-Systeme festgelegt. Eine der zentralen Pflichten, die
Unternehmen künftig im Umgang mit KI-Systemen
erfüllen müssen, ist die Vermittlung von KI-Kompe-
tenz: Mitarbeiter, die mit dem Betrieb und der Nutzung
von KI-Systemen befasst sind, sollen sich deren Fähig-
keiten, Unfähigkeiten, damit verbundenen Chancen und
Risiken, sowie möglicher Schäden, die sie verursachen
können bewusst werden.

4  HINWEIS

IKI-Systeme sind autonom. Autonomie bezeichnet die
Fähigkeit eines Systems, die Handlungsanweisungen
des menschlichen Programmierers aufgrund neuer
Daten eigenständig zu verändern.

2. Risiken des KI-Einsatzes

Trotz der Autonomie sind KI-generierte Inhalte kein
Ergebnis eines kreativen Schaffensprozesses, sondern
einer komplexen Wahrscheinlichkeitsrechnung. Auto-
nomie bedeutet, dass das System die Werte in dieser
Wahrscheinlichkeitsrechnung ohne menschlichen
Eingriff verändern kann. Inhalte eines Chatbots wie
ChatGPT sind aus Sicht des Nutzers sinnvoll zusammen-
hängende Texte, aus Sicht des Systems hingegen sta-
tistische Notwendigkeit. Die Texte geben daher mehr
oder weniger wahrscheinliche Wortketten.

BEISPIEL:

Auf die Wortkette „Ich wünsche dir einen Guten ..."
folgt wahrscheinlich als nächstes Wort „Morgen"
oder „Abend". Weniger wahrscheinlich ist hingegen
das Wort „Mittag", auch wenn der Satz „Ich wünsche
dir einen guten Mittag" inhaltlich und gramma-
tikalisch korrekt ist. Das führt bisweilen zu un-
erwünschten Ergebnissen. Die Systeme verknüpfen

sachlich falsche Informationen (Halluzination),
folgen Tendenzen, die Nutzereingaben zu ent-
nehmen sind und geben aufgrund einer entspre-
chenden Repräsentation in historischen Datensätzen
Antworten aus, die aus heutiger Sicht anstößig oder
gar verboten sind (Bias).

Das ungewünschte Ergebnis eines KI-Systems muss
nicht zwangsläufig zu Konsequenzen in der analogen
Welt führen. Wäre der Mensch stets in der Lage, diese
Phänomene zu erkennen und darauf zu reagieren,
könnte er entsprechende Ergebnisse aussortieren
oder überarbeiten. Er neigt aber dazu, der Technologie
zu vertrauen. In der Psychologie wird das als Auto-
matisierungsbias bezeichnet. Diesem unterliegend
übernehmen Autoren die falschen Vorschläge einer
automatisierten Rechtschreibprüfung oder folgen Auto-
fahrer ihrem Navigationssystem in den nächsten See.
Besonders bei Chatbots wird diese Gefahr virulent, da
die Programme ihr mangelndes Wissen durch überzeu-
gend klingende Texte kaschieren.

BEISPIEL:

Ein Anwalt in den USA fragte die KI nach Präzedenz-
fällen für ein Klageverfahren. Die KI nannte dem An-
walt mehrere vergleichbare Fälle mit Aktenzeichen.
Der zuständige Richter stellte im Laufe der Verhand-
lung fest, dass die zitierten Aktenzeichen von der KI
erfunden waren.

4  HINWEIS

Erkenntnis für den rechtssicheren Einsatz: KI ist
in manchen Kontexten verlässlich, sie ist allerdings
nie beherrschbar. Trotz ihrer Autonomie kann die
KI keine Verantwortung tragen. Vielmehr knüpft
die Rechtsordnung an die Verantwortlichkeit des
Individuums an. Zentraler Faktor des KI-Einsatzes im
Unternehmen muss deshalb der Mensch sein.

Was ist KI, 1  1 7

3 Wie kann KI eingesetzt

werden?

Ende 2022 ist ein autonomes KI-System für die breite Öffentlichkeit ver-
öffentlicht worden: ChatGPT. Es ist ein KI-System zur Verarbeitung natür-
licher Sprache (Natura! Language Processing). Das heißt, dass aufgrund
menschlicher Eingaben passende, weil wahrscheinliche Ausgaben produ-
ziert werden, die unserer Sprache nachgebildet sind. Doch die Abbildung
menschlicher Sprache ermöglicht nicht nur die Übersetzung oder das
Korrigieren menschlicher Texte: Wenn ein KI-System wahrscheinliche
Sprachtexte generieren kann, beginnt es auch dahinterstehende sprachli-
che Konzepte wie Logik, Emotionen oder Rhetorik nachzuahmen.

Was können diese KI-Systeme wirklich? Sogenannte generative KI-Sys-
teme wie ChatGPT werden als die ersten KI-Systeme bezeichnet, die einen
allgemeinen Verwendungszweck haben. Ein KI-System mit allgemeinem
Verwendungszweck kann zu nützlichen Zwecken eingesetzt werden, aber
auch zu schädlichen. Einige Beispiele werden in diesem Kapitel aufgeführt.

1. Einsatz zu nützlichen Zwecken

Die Verarbeitung natürlicher Sprache durch Computer
hat verschiedene Funktionantäten. Jeder dieser Funktio-
nalitäten können zahlreiche Anwendungsfälle zuge-
ordnet werden.

a) Generieren von Texten
Die wohl prominenteste Funktion der neuen Generation
von KI-Sprachsystemen ist das Generieren von Texten.
Dieser Anwendungsfall kann etwa vom Betreiber eines
Online-Shops genutzt werden:
• Kundenservice: KI-Chatbots können eine erste Hilfe
bei mehr oder weniger typischen Fragestellungen
bieten oder Bestellungen abwickeln.

• Öffentliche Kommunikation: Das Generieren von
Texten kann darüber hinaus beim Verfassen einer
Pressemitteilung oder der Konzeption einer Werbe-
kampagne helfen. Das System kann etwa einpräg-
same Werbeslogans vorschlagen.

• Website-Gestaltung: können mit KI automatisch

Produktbeschreibungen oder SEO-optimierte Texte
erstellen lassen, um Zeit zu sparen und die Sichtbar-
keit in Suchmaschinen zu verbessern.

b) Bearbeiten von Texten
Die KI-Sprachsysteme können auch für die Bearbeitung
eines fertiggestellten Textes eingesetzt werden. Anwen-
dungsfälle sind beispielsweise:
•  d i e  redaktionelle Überarbeitung;
•  d i e  Kürzung;
•  d i e  Übersetzung in andere Sprachen.

In diesen Fällen ist von einem Menschen zu prüfen,
ob das KI-System tatsächlich eine rein formelle Über-
arbeitung vorgenommen hat oder ob die sprachlichen
Änderungen mit einer Veränderung auf inhaltlicher
Ebene einhergehen.

FD• BEISPIEL:

Die KI soll einen Text über eine „Bank" korrigieren.
Der menschliche Nutzer sollte das Ergebnis dar-
aufhin überprüfen, ob die KI den richtigen Kontext
erfasst hat. Es könnte sein, dass die KI den Begriff
„Bank" in Zusammenhang mit einem Finanzinstitut
verstanden hat. Der Nutzer wollte hingegen einen
Text über Sitzgelegenheiten schreiben.

Suchen von Informationen

Bearbeiten von Texten

C

.

Funktionantäten von
KI-Sprachsystemen

Generieren von Texten

Verwalten von Informationen

Abbildung: Funktionalitäten von KI-Sprachsystemen

Wie kann KI eingesetzt werden? I  1 9

c) Suchen von Informationen
KI-Sprachsysteme können zum Suchen von Informa-
tionen in großen Dokumentenmengen eingesetzt
werden. Sie eignen sich daher etwa zum Wissensma-
nagement in einem Unternehmen.

Hier liegen große Schwächen beim Einsatz von
KI-Sprachsystemen: Weil sie nur wahrscheinliche
Ergebnisse generieren und nicht zwingend richtige Er-
gebnisse, müssen die gefundenen Informationen über-
prüft werden. Insbesondere bei der Suche nach schwer
auffindbaren Informationen laufen KI-Systeme Gefahr,
überzeugend, weil wahrscheinlich passende  a b e r  tat-
sächlich unrichtige Informationen zu liefern.

BEISPIEL:

Wird die KI gefragt, wann Olaf Scholz geboren ist,
wird das richtige Ergebnis „ui_ Juni 1958" genannt.
Bei weniger prominenten Personen, deren Geburts-
datum nicht auf Wikipedia und anderen bekannten
Quellen veröffentlicht ist, kann es sein, dass die KI
ein falsches Geburtsdatum „erfindet".

d) Verwalten von Informationen
Schließlich können KI-Sprachsysteme zum Verwalten
von Informationen eingesetzt werden.
•  E i n  Handwerksbetrieb kann sich von einem KI-

System Aufträge sortieren und auf dieser Basis einen
Schichtplan erstellen lassen.

•  E b e n s o  könnte sich ein Immobilienverwalter von

einer KI ein effizientes System zur Dokumentenver-
waltung vorschlagen und einrichten lassen.

2. Einsatz zu schädlichen Zwecken

Diese nützlichen Zwecke zeigen, dass KI-Systeme ein
enormes Potenzial bieten. Allerdings auch für schäd-
liche und illegale Zwecke: Hacker nutzen KI-gestützte
Werkzeuge, um Sicherheitslücken zu finden, Passwörter
zu knacken oder Schadsoftware zu entwickeln. Auch
das Versenden von personalisierten Spam-Nachrichten
wird durch KI erleichtert. Beispielsweise enthalten
Phishing-Mails eine persönliche Anrede, einen gram-
matikalisch einwandfreien Text und täuschend echt
aussehende Links und Anhänge. Besonders gefährlich
ist die Möglichkeit, dass KI-Systeme detaillierte Anlei-
tungen zu illegalen Aktivitäten wie dem Bau von Waffen
oder Bomben erstellen. Selbst wenn der Zugang zu sol-
chen Inhalten eingeschränkt wird und die Ausgabe der
Informationen durch Filter verhindert wird, besteht die
Gefahr, dass sie in falsche Hände geraten und zu Gewalt-
taten führen. Zudem hat der enorme Energieverbrauch
beim Training und Einsatz großer KI-Modelle negative
Auswirkungen auf die Umwelt.

Unternehmen sehen sich ebenfalls bedroht, da An-
greifer durch bestimmte Angriffstechniken vertrauliche
Daten aus KI-Modellen extrahieren können, die wäh-
rend des Trainings verwendet wurden. Dies kann zur
Offenlegung von Geschäftsgeheimnissen oder Kunden-
daten führen. Eine weitere gefährliche Anwendung ist
die Erstellung überzeugend klingender „Fake News", die
in sozialen Medien schnell viral gehen und die öffent-
liche Meinung manipulieren können.

Die Liste der schädlichen KI-Anwendungen ließe sich
beliebig fortsetzen: von gefälschten Bildern und Videos,
sogenannte „Deepfakes", bis hin zum automatisierten
Betrug und der Manipulation von Wahlen. Es ist daher
zentral, dass der Einsatz dieser Technik achtsam erfolgt
und die potenziellen Gefahren im Blick behalten werden.

2 0   I   W i e   k a n n  KI elnget.etzt  w e i d e n )

4 Welche Pflichten hat der

Betreiber nach der KI-VO?

KI birgt erhebliche Potenziale, aber auch große Risiken für Bürger und
Gesellschaft. Diese Risiken versucht der europäische Gesetzgeber mit
seinem Gesetz über künstliche Intelligenz, die sogenannte KI-Verordnung
(KI-VO) zu verringern. Die KI-VO folgt einem risikobasierten Ansatz: Je
risikoreicher der Zweck, für den ein KI-System eingesetzt wird, desto
strenger wird geregelt, ob und unter welchen Voraussetzungen es auf den
Markt kommen darf Einige KI-Systeme sind nach der KI-VO folglich
verboten, für hochriskante Systeme gelten strenge Vorgaben und einfache
Systeme unterliegen keiner besonderen Regulierung. Der risikoorientierte
Ansatz der KI-VO leuchtet ein: Warum sollten für das Schreiben einer
Geburtstagskarte mit ChatGPT die gleichen Voraussetzungen gelten, wie
für das Abfassen eines Urteils oder die Bewertung einer Bachelorarbeit? In
diesem Kapitel wird beschrieben, welche Pflichten Unternehmen und
Behörden konkret erfüllen müssen, wenn sie eingekaufte KI-Systeme in
eigener Verantwortung verwenden und damit in der Sprache der KI-VO
Betreiber sind. Im Gegensatz zu Anbietern, Händlern und Einführern, die
KI-Systeme zwar entwickeln, importieren, verkaufen oder in Verkehr
bringen, ist es der Betreiber, der letztendlich den konkreten Einsatz und die
Auswirkungen des KI-Systems realisiert.

r

1. Verbotene Zwecke

Einige Praktiken im KI-Bereich sind nach Einschätzung
des europäischen Gesetzgebers so gefährlich, dass sie
grundsätzlich verboten wurden. Dazu zählen etwa die
unterschwellige Beeinflussung oder die Bewertung natür-
licher Personen anhand ihres sozialen Verhaltens (so-
genanntes Social Scoring) durch autonome KI-Systeme.
Während die meisten dieser Verbote in erster Linie im
staatlichen Kontext relevant werden und den typischen
Betreiber eines KI-Systems kaum betreffen, ist einer der
verbotenen Einsatzzwecke von besonderer Bedeutung:
Verboten sind die Inbetriebnahme und die Verwendung
eines KI-Systems zur Ableitung von Emotionen einer
natürlichen Person am Arbeitsplatz und in Bildungsein-
richtungen. KI-gestützte Maßnahmen zur Feststellung
des Betriebsklimas oder von Prüfungsangst sind damit
unzulässig. Bei Verstößen drohen hohe Bußgelder. Eine
Ausnahme gilt nur für den Einsatz des Systems aus
medizinischen oder Sicherheitsgründen. So könnte bei-
spielsweise ein KI-System, das zur Vermeidung des Sekun-
denschlafs bei LKW-Fahrern, deren Gesichtszüge während
der Fahrt analysiert nicht zwingend verboten sein.

Wo steht es?
Art.5 KI-VO

2. Hochriskante Zwecke

Ist ein System nicht verboten, aber hochriskant, werden
strenge Anforderungen an die Entwicklung und den
Betrieb gestellt. Was genau aber ist unter hochriskanten
KI-Systemen zu verstehen?

Die meisten Fälle, in denen ein KI-System als hochris-
kant eingestuft wird, sind in der Verordnung aufgeführt.

Sie umfassen folgende Bereiche:
• Biometrie;
•  k r i t i s c h e  Infrastruktur;
• allgemeine und berufliche Bildung;
• Beschäftigung, Personalmanagement und Zugang

zur Selbstständigkeit;

• Zugänglichkeit und Inanspruchnahme grund-

legender privater und grundlegender öffentlicher
Dienste und Leistungen;

• Strafverfolgung;
• Migration, Asyl und Grenzkontrolle;
• Rechtspflege und demokratische Prozesse:

Wo steht es?
Art.6 KI-VO in Verbindung mit Anhang III

a) KI-Einsatz im hochriskanten Bereich
Um diesen abstrakten Begriffen etwas Leben einzuhau-
chen, seien hier einige Anwendungsfälle beschrieben:

In einer Studie der Universität Cambridge wurde fest-
gestellt, dass das einem KI-System auf Basis des Sprach-
modells GPT-4, auf dem auch ChatGPT beruht, bei der
Beurteilung von Augenproblemen und der Beratung
von Patienten mehr zuzutrauen sei als Ärzten. Genau
genommen schnitt das KI-System nur im Vergleich zu
unerfahrenen Assistenzärzten besser ab, die Leistung
ist aber dennoch beachtlich. Sie verleitete die For-
scher in einer Pressemitteilung zu der Aussage, dass
KI realistischerweise beim Triagieren von Patienten
mit Augenproblemen eingesetzt werden könne, um zu
entscheiden, wann ein Notfall Priorität hat. Hier wird
es komplex. Wie beschrieben sind KI-Systeme autonom
und damit nicht beherrschbar. Bei der Triage geht es
darum, Überlebenswahrscheinlichkeiten in Notfällen
zu berücksichtigen. Die komplexen Wertungen hinter
der Triage können schwerlich allein per KI vorge-
nommen werden, die in ihrer Unbeherrschbarkeit aus
Trainingsdatensätzen möglicherweise diskriminierende
Folgerungen abgeleitet hat. Deshalb wertet die KI-VO
die Triage berechtigterweise als hochriskanten Zweck
des KI-Einsatzes. Bereits der Entwickler des Systems
muss deshalb sicherstellen, die Qualität des KI-Systems
sowie das vorzeitige Erkennen und Abstellen möglicher
Risiken durch technische und organisatorische Maß-
nahmen sicherzustellen. Der Anwender darf es nur für
die vorgeschriebenen Zwecke nutzen und dabei nicht
übermäßig in die von dem System hervorgebrachten
Ergebnisse vertrauen.

Erhebliche Auswirkungen auf betroffene Personen kann
auch die Prüfungsbewertung mittels eines KI-Systems
haben. Im Vereinigten Königreich wurden wegen der Co-
rona-Pandemie Abschlussprüfungen für Schulabgänger
gestrichen. An deren Stelle berechnete ein KI-System die
Abschlussnote. In der Folge erhielten tausende Schüler,

2 2  1 Welche Pflichten hat der Betreiber nach der KINO?

insbesondere aus sozial schwachen Verhältnissen,
schlechtere Noten, als ihre Lehrer antizipiert hatten.
Sicher geglaubte Studienplätze gingen dadurch zunächst
verloren. Die Regierung sah sich daraufhin gezwungen,
die berechneten Noten zurückzurufen und die Beurtei-
lung den jeweiligen Lehrkräften zu überlassen. Das
Beispiel zeigt, dass auch bei der Leistungsbewertung
strenge Anforderungen an den KI-Einsatz zu stellen
sind — so sieht es auch die KI-VO vor.

Die KI-VO qualifiziert allerdings nicht alle gefährlichen
Anwendungsmöglichkeiten künstlicher Intelligenz als
hochriskant. KI-Systeme können erheblichen Einfluss
auf unsere Meinungsbildung haben. Indem genera-
tive Systeme wie ChatGPT Texte und Bilder erzeugen,
vermitteln sie den Eindruck eines faktenbasierten
Austauschs von Informationen. Weil dahinter aber nur
die wahrscheinlichste Wortfolge oder Pixelzusammen-
setzung auf Basis vergangener Daten ausgegeben wird,
spiegeln diese Systeme nicht unsere Welt, sondern ihr
Training wider  e i n  vom Internet geprägter Ausschnitt
der Realität. Lässt man ChatGPT aktuelle gesellschaft-
liche Fragen beantworten, ergibt sich also eine poli-
tische Tendenz auf Basis der Trainingsdaten aus dem
Internet, mit der Bürger bei der Nutzung des Systems
konfrontiert werden. Trotzdem gilt der KI-Einsatz im
Bereich der Meinungsbildung nicht als hochriskant.
Im Bereich demokratischer Prozesse sind die Anforde-
rungen an hochriskante KI-Systeme zwar zu erfüllen.
Konkret sind damit aber lediglich Systeme angespro-
chen, die dazu verwendet werden sollen, das Ergebnis
einer Wahl oder das Wahlverhalten des Einzelnen zu
beeinflussen. Vielfaltssichernde Regelungen für KI-
Systeme sieht die KI-VO hingegen nicht vor.

Die Sicherung der Meinungsvielfalt im Internet — ge-
gebenenfalls durch KI-Systeme — ist Aufgabe anderer
europäischer Gesetze, etwa des Digital Services Acts.

BEISPIELE

• Biometrie:

Leseunterstützung mit automatischen Über-
setzungen und Begriffsklärungen aufgrund von
Pupillenerweiterung

• Kritische Infrastruktur:

Automatisierte Routenplanung für die Müll-
abfuhr in einer Kommune

• Allgemeine und berufliche Bildung:

KI-gestützte Analyse von Schülerverhalten, um
deren Eignung oder Lernfähigkeit zu bewerten

• Beschäftigung, Personalmanagement und

Zugang zur Selbstständigkeit:
KI-basierte Analyse von Soft Skills in Bewerbungs-
videos für ein Praktikum

• Zugänglichkeit und Inanspruchnahme grund-
legender privater und öffentlicher Dienste und
Leistungen:
Automatisierte Priorisierung von Anträgen in der
Hotline eines Stromanbieters

• Strafverfolgung:

Einsatz von KI zur Erkennung von verdächtigem
Verhalten in Überwachungsvideos eines Einkaufs-
zentrums

• Migration, Asyl und Grenzkontrolle:

Automatische Prüfung der Vollständigkeit von
Anträgen

• Rechtspflege und demokratische Prozesse:
KI-unterstützte Erstellung von Sitzungsproto-
kollen in einem Gemeinderat

Ausnahmsweise Risikominderung
Die beschriebenen Einsatzzwecke führen aber nicht
immer zu einer besonderen Regulierung durch die KI-
VO. Wenn das System die menschliche Entscheidungs-
findung nicht wesentlich beeinflusst, gilt der Einsatz
auch in den genannten Bereichen nicht als hochriskant.

Das ist der Fall, wenn das System dazu bestimmt ist:
•  e i n e  eng gefasste Verfahrensaufgabe durchzuführen;
•  d a s  Ergebnis einer zuvor abgeschlossenen mensch-

lichen Tätigkeit zu verbessern;

• Entscheidungsmuster oder Abweichungen von frü-
heren Entscheidungsmustern zu erkennen, oder
•  e i n e  vorbereitende Aufgabe für eine Bewertung

durchzuführen, die für hochriskante Zwecke rele-
vant ist.

Für die oben genannten Beispiele für Hochrisiko-
Systeme wie die KI-gesteuerte Routenplanung für die

Welche Pflichten hat der Betreiber nach der KI-VO? 1 23

Müllabfuhr bedeutet das Folgendes: Wenn das System
nur Empfehlungen gibt, die von Mitarbeitern überprüft
und gegebenenfalls angepasst werden, ist der Einsatz
nicht hochriskant. Auch bei der intelligenten Lernsoft-
ware, die den Lernstil eines Schülers erkennt, wäre der
Einsatz nicht hochriskant, solange diese Information
nur dem Betroffenen mitgeteilt wird, um besser zu
lernen und kein Lehrer hiervon erfahrt, sodass eine
Bewertung ausgeschlossen ist. Bei der Priorisierung
von Anfragen in der Hotline eines Stromanbieters gilt
Ähnliches: Wenn die KI nur unterstützend wirkt und
die wesentliche Entscheidung letztlich durch einen
Menschen getroffen wird, ist der Einsatz nicht hoch-
riskant. Im Einzelfall ist die Grenze zwischen einer rein
formalen Unterstützung und einer wesentlichen Beein-
flussung der Entscheidungsfindung oft fließend, was
die Abgrenzung erschwert. Es wird von der konkreten
Ausgestaltung des Systems, der Art der Entscheidung
und dem Kontext des Einsatzes abhängen. Wer sich auf
einen der genannten Ausnahmetatbestände beruft,
muss eine entsprechend eingeschränkte Verwendung
des KI-Systems dokumentieren und auf Verlangen der
Behörden nachweisen.

Wie praxistauglich die Ausnahmevorschrift ist, wird
sich daher zeigen müssen. Die Anwendungsfälle, die
dem europäischen Gesetzgeber bei der Schaffung der
Norm vorschwebten, können oftmals ebenso von einfa-
cher, nicht-autonomer Software übernommen werden.
Unternehmen und Behörden sollten sich daher fragen,
ob für den konkreten Anwendungsfall ein KI-System
angeschafft werden soll, das nur unter der Bedingung
einer umfangreichen Dokumentation nicht als hochris-
kant zu bewerten ist, oder ob sie die Aufgabe lieber einer
klassisch programmierten Software übertragen, auf
welche die KI-VO keine Anwendung findet.

Wo steht es?
Art. 6 Abs.3 KI-VO

3. Betreiberpflichten für
Hochrisiko-KI-Systeme

Betreiber von Hochrisiko-KI-Systemen müssen zwar
nicht die umfassenden Anforderungen an die Entwick-
lung und das Inverkehrbringen der Systeme erfüllen.

Auch sie treffen aber besondere Pflichten, die einen
sicheren Einsatz gewährleisten sollen.

a) Allgemeine Betreiberpflichten
Zunächst hat der Betreiber mindestens einem Mitarbei-
tenden die Aufsicht über das Hochrisiko-KI-System zu
übertragen. Die menschliche Aufsicht muss über die
erforderliche Kompetenz. Ausbildung und Befugnis ver-
fügen: Sie muss also insbesondere in der Lage sein, die
Gefahr eines Automatisierungsbias zeitnah zu erkennen
und Maßnahmen zu ergreifen, die ein übermäßiges Ver-
trauen in die Ergebnisse des Systems verhindern. Der
menschlichen Aufsicht muss auch die Befugnis über-
tragen werden, den Betrieb des Hochrisiko-KI-Systems
zu stoppen, wenn sie unbeherrschbare Risiken erkennt.
Eine praktikable Ausgestaltung dieser Befugnis erfor-
dert auch eine gewisse Unabhängigkeit der mensch-
lichen Aufsicht. Andernfalls besteht die Gefahr, dass
den Betrieb beschränkende Maßnahmen aus Angst vor
Repressalien nicht ergriffen werden.

Erster Schritt — Datenqualität: Betreiber müssen auch
sicherstellen, dass Daten, die in das KI-System einge-
speist oder von diesem erfasst werden, der Zweckbe-
stimmung des Systems entsprechen und ausreichend
repräsentativ sind.

Zweiter Schritt— Dokumentation: Protokolle, die das
System erzeugt, sind von dem Betreiber grundsätzlich
für mindestens sechs Monate aufzubewahren.

Dritter Schritt — Information und Transparenz: Sofern
ein KI-System am Arbeitsplatz eingesetzt wird, etwa
um Abläufe zu optimieren oder Schichtpläne effizient
zu erzeugen, muss der Arbeitgeber die Arbeitnehmer-
vertreter und die betroffenen Arbeitnehmer darüber
informieren, dass sie der Verwendung eines Hochri-
siko-KI-Systems unterliegen werden.

Gleiches gilt gegenüber anderen betroffenen Personen,
die keine Mitarbeitenden des Betreibers sind. Wird also
etwa ein KI-System eingesetzt, um Bewerbungen nach
vorbestimmten Kriterien auszuschließen, sind Be-
werber über den Einsatz des Systems zu informieren.

Vierter Schritt — Kontinuierliche Überwachung:
Schließlich sind Betreiber dafür verantwortlich, den

2 4  I  We l c h e  Pthchten hat der Betreiber nach der KI-VO?

ordnungsgemäßen Betrieb des Hochrisiko-KI-Systems
zu überwachen. Stellen sie dabei fest, dass das einge-
setzte KI-System ein Risiko für die Gesundheit, Sicher-
heit oder Grundrechte birgt, müssen sie den Anbieter
oder Händler und die zuständige Behörde informieren
und die Verwendung des Systems aussetzen. Gleiches
gilt, wenn ein schwerwiegender Vorfall festgestellt wird.
Ein schwerwiegender Vorfall liegt vor, wenn der Betrieb
eine besonders schwere Folge hat, zum Beispiel den Tod
oder die schwere gesundheitliche Schädigung einer
Person oder eine schwere und unumkehrbare Störung
der Verwaltung und des Betriebs kritischer Infrastruk-
turen.

Konkret heißt das: Um sicherzustellen, dass der Betrieb
gemäß diesen Anforderungen erfolgt, hat der Betreiber
geeignete technische und organisatorische Maß-
nahmen zu treffen. Dabei hat er auch die Vorstellungen
des Anbieters von einem ordnungsgemäßen Betrieb zu
beachten. Für welchen Einsatz der Anbieter das jeweilige
KI-System entwickelt hat, ergibt sich aus der Betriebs-
anleitung, die Anbieter den Betreibern zur Verfügung
stellen müssen. Zu organisatorischen Maßnahmen
zählen etwa Anweisungen an die Mitarbeitenden, in
denen die von Arbeitgeberseite erlaubten Einsatzzwecke
klar definiert sind, oder die angesprochene Unabhängig-
keit der menschlichen Aufsicht.

b) Grundrechte-Folgenabschätzung durch
öffentliche Stellen
Handelt es sich bei dem Betreiber um eine Einrichtung
des öffentlichen Rechts oder um eine private Ein-
richtung, die öffentliche Dienste erbringt, ist darüber
hinaus eine Grundrechte-Folgenabschätzung erforder-
lich. Diese setzt voraus, dass der vorgesehene Einsatz
klar definiert wird. Dazu zählt eine Beschreibung der
Zweckbestimmung und des Verfahrens sowie des Zeit-
raums und der Häufigkeit der Verwendung des Sys-
tems. Der Betreiber muss sich darüber im Klaren sein,
welche Personengruppen von dem Einsatz betroffen
und welche Schäden zu befürchten sind. Es genügt dazu
nicht, abstrakte Überlegungen zu Schadensrisiken an-
zuführen. Vielmehr muss sich der Betreiber im Einzel-
fall darüber im Klaren sein, ob bei einschneidenden
Lebensereignissen wie staatlichen Prüfungen, der
Beantragung sozialer Leistungen zur Existenzsicherung
oder der Frage über den Zugang zur Daseinsvorsorge die

Unbeherrschbarkeit des jeweiligen KI-Systems akzep-
tabel ist. Zu einer vollständigen Folgenabschätzung
zählt schließlich auch eine Auseinandersetzung mit der
Frage, wie auf Beschwerden reagiert werden kann.

Während die vorbenannten Pflichten das „Wie" des
Einsatzes betreffen, soll sich der öffentliche Sektor im
Rahmen der Grundrechte-Folgenabschätzung darüber
Gedanken machen, „ob" der konkrete Einsatz grund-
rechtskonform ist.

4  HINWEIS

Das heißt auch, dass Behörden auf bestimmte Ein-
sätze verzichten müssen, bis sie die Grundrechte-Fol-
geabschätzung getroffen haben. Wenn eine Schule
zur Bewertung von Prüfungsleistungen ein hochris-
kantes KI-System einsetzen möchte, ist das rechtlich
erst erlaubt, wenn diese Grundrechte-Folgenabschät-
zung mit einem positiven Ergebnis abgeschlossen
werden konnte.

Die Grundrechte-Folgenabschätzung hat damit das Ziel
einer behördlichen Vergewisserung: Ist der konkret
geplante Einsatz mit den Beeinträchtigungen für die
Grundrechte betroffener Personen vereinbar? Wird die
KI-basierte Benotung die Chancengleichheit berühren
und wie lässt sich das verhindern? Möglicherweise geht
das bei einem autonomen System nur dadurch, dass
man es zu diesem Zweck nicht einsetzt. Die Folgenab-
schätzung würde dann negativ ausfallen und der Einsatz
müsste ausgeschlossen sein.

4. Transparenzpflichten

Bei der Verwendung von Hochrisiko-KI-Systemen haben
Betreiber die betroffenen Personen über den Einsatz zu
informieren. Für zulässige Emotionserkennungssys-
teme und Systeme zur biometrischen Kategorisierung
gilt zudem eine spezielle Informationspflicht. Daneben
sieht die KI-VO Transparenzpflichten für bestimmte ge-
nerative KI-Systeme vor. So müssen die Betreiber eines
KI-Systems, das Bild-, Ton- oder Videoinhalte erzeugt
oder manipuliert offenlegen, dass die Inhalte künstlich
erzeugt oder manipuliert wurden. Zur Aufdeckung von
Straftaten soll diese Pflicht nicht gelten, im Kontext
offensichtlich künstlerischer, kreativer, satirischer oder

Welche Pflichten hat der Betreiber nach der KI-VO? 1  2 5

fiktionaler Betätigung nur eingeschränkt, sodass die
Darstellung oder der Genuss des Werkes nicht beein-
trächtigt wird. Gerade bei der Erzeugung und Manipula-
tion von Bild-, Ton- oder Videoinhalten haben Betreiber
aber auch die allgemeinen Gesetze zu beachten.

BEISPIEL

Wer ein satirisches Video generieren lässt, in dem
der Bundeskanzler ein Parteiverbotsverfahren
ankündigt, muss sich daher nicht nur mit der
(eingeschränkten) Transparenzpflicht der KI-VO aus-
einandersetzen, sondern auch mit den komplexen
Wechselwirkungen zwischen Kunstfreiheit und
Persönlichkeitsrecht.

4  HINWEIS

Eine weitere Transparenzpflicht trifft zuvorderst
Zeitungsverlage und Journalisten: Wenn ein KI-gene-
rierter Text veröffentlicht wird, um die Öffentlichkeit
über Angelegenheiten von öffentlichem Interesse zu
informieren, muss der Betreiber offenlegen, dass der
Text künstlich erzeugt oder manipuliert wurde.

Formulierungsvorschlag: Dieser Inhalt ist mit Unter-
stützung eines KI-System erstellt worden.

Die Pflicht gilt ausnahmsweise nicht, wenn KI-gene-
rierte Inhalte von einem Menschen überprüft werden
und das Unternehmen oder ein Mitarbeiter die redak-
tionelle Verantwortung für die Veröffentlichung der
Inhalte trägt.

HINWEIS
Derzeit ist es mit der journalistischen Sorgfaltspflicht
ohnehin kaum vereinbar, KI-generierte Inhalte
ohne menschliche Überprüfung zu veröffentlichen.
Daneben ist von einer solchen Praxis auch wegen der
bestehenden Haftungsrisiken abzuraten, auf die zu
einem späteren Punkt genauer einzugehen sein wird.
Die entsprechende Transparenzpflicht der KI-VO
dürfte daher jedenfalls im journalistischen Kon-
text bisher keine tragende Rolle spielen. Außerhalb
des Journalismus wird diese Pflicht für Behörden

relevant, wenn sie behördliche Informations- oder
Gefahrenmeldungen von einem KI-System gene-
rieren lassen. Nur für die Zwecke der Strafverfolgung
werden Behörden von der Transparenzpflicht befreit.

Wo steht es?
Art. 50 KI-VO

5. Wechsel der Pflichten: Vom
Betreiber zum Anbieter per
Zweckänderung

Die strengen Anforderungen der KI-VO an hochris-
kante KI-Systeme sind vor allem von den Anbietern zu
erfüllen. Sie haben sicherzustellen, dass die Systeme
so konzipiert und entwickelt sind, dass sie die Grund-
rechte Betroffener nicht verletzen, der Gesundheit
nicht schaden und den gesellschaftlichen Interessen
nicht zuwiderlaufen. Erst auf der nachfolgenden Stufe
werden Unternehmen und Behörden adressiert, die ein
KI-System einsetzen. Doch auch diese Betreiber und ihr
Personal können mit dem umfangreichen Pflichtenka-
talog der Anbieter konfrontiert werden.

Sie haben die Anbieterpflichten für Hochrisiko-KI-
Systeme zu erfüllen, wenn:

•  s i e  ein existierendes Hochrisiko-KI-System mit

ihrem Namen oder ihrer Handelsmarke versehen;

BEISPIEL

Ein US-amerikanisches Softwareunternehmen bietet
eine KI an, die Bewerbungen nach der ausgeschrie-
benen Stellenanzeige auswertet. Ein Unternehmen
mit Sitz in Deutschland erwirbt Lizenzen für diese KI
und vertreibt sie unter eigenem Namen an deutsche
Kunden.

•  s i e  eine wesentliche Veränderung eines Hochrisiko-
KI-Systems so vornehmen, dass es ein Hochrisiko-
KI-System bleibt;

2 6  1 Welche Pflichten hat der betreibe, nach der KI-VO)

BEISPIEL

BEISPIEL

Ein US-amerikanisches Softwareunternehmen bietet
eine KI an, die Bewerbungen nach der ausgeschrie-
benen Stellenanzeige auswertet. Ein Unternehmen
mit Sitz in Deutschland passt die KI an die rechtlichen
Anforderungen, unter anderem an das Allgemeine
Gleichbehandlungsgesetz (sogenanntes Antidiskrimi-
nierungsgesetz) für den deutschen Markt an.

•  s i e  die Zweckbestimmung eines KI-Systems, das
nicht als hochriskant eingestuft wurde so verän-
dern, dass das betreffende System zu einem Hoch-
risiko-KI-System wird.

BEISPIEL

Die Personalabteilung eines Unternehmens nutzt
ChatGPT für die Auswahl von Bewerbern, indem
die Stellenanzeigen und die eingegangenen Be-
werbungsunterlagen in das KI-System hochgeladen
werden.

In all diesen Fällen geht die Anbieterrolle auf den Be-
treiber über. Der ursprüngliche Anbieter ist dann nur
noch verpflichtet, Informationen zur Verfügung zu
stellen und für technischen Zugang sowie sonstige er-
wartbare Unterstützung zu sorgen. Dem neuen Anbieter
obliegt es hingegen unter anderem, die Konzeption des
KI-Systems zu überwachen und an die Anforderungen
der KI-VO anzupassen. Das kann für Unternehmen und
Behörden ohne das entsprechende Know-how zu einer
schier unüberwindbaren Herausforderung werden.
Nach Möglichkeit sollten sie einen Wechsel in die An-
bieterrolle also unbedingt vermeiden.

—> HINWEIS: WIE KÖNNEN BETREIBER DAS VER-

MEIDEN?
Es kann keine Lösung sein, sich nicht mit der neuen
Technologie zu beschäftigen. Mitarbeitende werden
die Systeme zu beruflichen Zwecken auf privaten
Accounts einsetzen. Die KI-VO sieht zwar eine Be-
reichsausnahme für die Privatnutzung vor. Diese
greift aber nur, wenn das entsprechende System zu
ausschließlich privaten Zwecken eingesetzt wird.

•  F ü r  eine Feier zum 50. Geburtstag eines Fami-

lienmitglieds soll die KI eine Glückwunschkarte
schreiben.

•  N u t z t  der Mitarbeitende seinen privaten Account,

um eine geschäftliche E-Mail zu übersetzen,
sind die Regelungen der KI-VO zu beachten. Lässt
sich der Betreiber also nicht zur Nutzbarkeit
von KI-Systemen ein, läuft er Gefahr, aufgrund
der faktischen Nutzung der Systeme durch die
Mitarbeitenden den Pflichten der KI-VO unter-
worfen zu werden, ohne dass er darauf Einfluss
nehmen könnte. Betreiber sollten den Einsatz
also entweder allgemein verbieten oder Systeme
lizenzieren und erlaubte (nicht hochriskante) Ein-
satzzwecke für ihre Mitarbeitenden definieren.
Eine dritte Möglichkeit besteht darin, ein Hoch-
risiko-KI-System zu lizenzieren.

Weist ein Anbieter sein KI-System beispielsweise als
System zur Prüfungsbewertung aus, hat er die darauf
bezogenen Anbieterpflichten selbst zu erfüllen. Eine
Hochschule könnte das System dann zur Prüfungsbe-
wertung einsetzen lassen und bliebe trotzdem Betrei-
berin. Der Anbieter wird es sich bezahlen lassen, dass er
die Anbieterpflichten zu erfüllen hat. Mittelfristig kann
dieser Weg für Betreiber dennoch wirtschaftlich vorteil-
haft sein, denn die Umsetzung der Anforderungen an
hochriskante KI-Systeme ist ebenfalls mit erheblichen
Kosten verbunden.

HINWEIS
Für Arbeitnehmer folgt aus dieser Regelung eine
dringliche Empfehlung: Hat der Arbeitgeber den KI-
Einsatz zu hochriskanten Zwecken verboten, sollte
dieser Weisung in jedem Fall gefolgt werden. Wer sich
widersetzt und ein System dennoch im Hochrisiko-
bereich, beispielsweise im Personalmanagement, ein-
setzt, befindet sich im Exzess. Die Anbieterpflichten
treffen dann den Arbeitnehmer, der im Regelfall noch
weniger Möglichkeiten zur Erfüllung hat als der Be-
treiber. In diesem Fall drohen hohe Bußgelder.

Wo steht es?
Art. 25 KI-VO

Welche Pflichten hat der Betreiber nach der KI-310? 1  2 7

6. KI-Kompetenz

Die Betreiber müssen schließlich sicherstellen, dass ihr
Personal und alle anderen Personen, die in ihrem Auf-
trag mit dem Einsatz der Systeme befasst sind, über ein
ausreichendes Maß an KI-Kompetenz verfügen. Diese
Pflicht gilt unabhängig von der konkreten Risiko-
bewertung. Sie ist damit nicht auf die Betreiber von
Hochrisiko-KI-Systemen beschränkt, sondern gilt auch
für die Betreiber von KI-Systemen mit allgemeinem Ver-
wendungszweck wie ChatGPT.

„KI-Seepferdchen": Diese Rechtspflicht trifft nach der
KI-VO alle. Unabhängig davon, wie riskant der KI-Einsatz
ist. Daraus folgt eine Schulungspflicht für die Allge-
meinheit.

Unter den Begriff der KI-Kompetenz fallen das allge-
meine Verständnis sowie alle Fähigkeiten und Kennt-
nisse, die es ermöglichen, KI-Systeme sachkundig
einzusetzen. Betreiber müssen also insbesondere dafür
sorgen, dass ihren Mitarbeitenden die Chancen und Ri-
siken von KI sowie mögliche Schäden bewusst werden.
Bei der Entwicklung entsprechender Lehrmaßnahmen
haben die Betreiber die technischen Kenntnisse, die
Erfahrung, die Ausbildung und die Schulung ihrer Mit-
arbeitenden sowie die konkrete Form des KI-Einsatzes
zu berücksichtigen.

Mitarbeiterschulungen zu KI sollten insbesondere
folgende Themen beinhalten:
•  W a s  ist KI?
• Welche  Software im Arbeitsalltag enthalten KI?
•  W i e  nutze ich KI richtig?
•  W i e  funktioniert prompten?
• Welche  Chancen und Risiken bestehen beim Einsatz

von KI?

Neben technischen und ethischen Aspekten umfasst die
KI-Kompetenz auch ein grundlegendes Verständnis der
geltenden Regulierungsmaßnahmen. Im Rahmen der
KI-VO sind das im Wesentlichen die in diesem Kapitel
beschriebenen Pflichten und Regelungen. Dazu zählt
auch die angesprochene Gefahr, selbst zum Anbieter
eines Hochrisiko-KI-Systems zu werden, wenn einfache
KI-Systeme weisungswidrig zu hochriskanten Zwecken
eingesetzt werden. Die KI-VO regelt den KI-Einsatz aber

nicht abschließend. Daneben sind die allgemeinen
Gesetze zu beachten, die in den folgenden Kapiteln

dargestellt werden.

4  HINWEIS
IInsgesamt bietet diese Broschüre einen guten An-

haltspunkt dafür, was die KI-VO von den Betreibern
bei der Vermittlung von KI-Kompetenz verlangt.

Wo steht es?
Art.4 KI-VO („Seepferdchen-Artikel")

7. Bestandsschutz für „alte" KI

Viele Unternehmen, Behörden und Vereine setzen be-
reits heute verschiedene KI-Systeme ein. Doch mit dem
geplanten Inkrafttreten der europäischen KI-VO stellt
sich die Frage: Was passiert eigentlich mit den KI-Sys-
temen, die schon vor der Verordnung entwickelt und
genutzt wurden? Hier kommt der sogenannte Bestands-
schutz ins Spiel.

Der Bestandsschutz bedeutet vereinfacht gesagt, dass
bestimmte KI-Systeme, die vor dem Geltungsbeginn der
KI-VO in Betrieb waren, von den neuen Anforderungen
ausgenommen sind. Das gilt insbesondere für Hoch-
risiko-KI-Systeme, also solche, die in sensiblen Berei-
chen eingesetzt werden und potenziell großen Schaden
anrichten können. Dazu zählen zum Beispiel KI-Anwen-
dungen im Personalwesen oder im Bildungsbereich.

ACHTUNG

Der Bestandsschutz gilt nur, wenn die Konzep-
tion des KI-Systems im Wesentlichen gleich-
bleibt; genau hier liegt bei KI-Systemen aber ein
Problem. Denn anders als bei Kinderspielzeugen
oder Kosmetikprodukten ist die ständige Weiter-
entwicklung bei KI-Systemen oft von vornherein
angelegt. Eine ihrer Kerneigenschaften als auto-
nomes System ist, selbstständig dazu zu lernen
und sich an neue Gegebenheiten anzupassen. Die
Grenze zwischen einer normalen Fortentwicklung
und einer wesentlichen Veränderung ist da schnell
verwischt.

2 8  1 Welche Pflichten hat der Betreiber nach der KI-VO?

Hinzu kommt, dass manche KI-Systeme von Anfang an
für allgemeine Zwecke konzipiert sind, das heißt die
KI kann sowohl für Übersetzungen, Textgenerierung
als auch Korrekturen eingesetzt werden. Dabei spielt
es keine Rolle, ob die generierten Texte von Schülern,
AMgälten, Geschäftsführern oder Musikern verwendet
werden. Auch hier fällt es schwer zu beurteilen, ob ein
neuer Einsatzzweck noch vom Bestandsschutz gedeckt
ist oder nicht. Es besteht die Gefahr, dass findige An-
bieter und Nutzer den Bestandsschutz als Schlupfloch
nutzen, um die strengen Anforderungen der KI-VO zu
umgehen. Indem sie die Systeme noch schnell vor dem
Stichtag in Betrieb nehmen, könnten sie behaupten:
„Das war doch alles schon von dem ursprünglichen Ein-
satzzweck abgedeckt. Wir haben da nichts Wesentliches
verändert".

der für die meisten Anwendungsszenarien ausreicht,
könnte es verlockend sein, sie einfach unverändert
weiter zu nutzen. Geht der rasante Fortschritt in der
KI-Entwicklung aber unvermindert weiter, wäre es
wirtschaftlich fahrlässig, auf einem veralteten Stand zu
verharren  a u c h  wenn die Umsetzung der KI-Verord-
nung Kosten nach sich zieht.

HINWEIS
Wer sich heute und in naher Zukunft für den Einsatz
von KI-Systemen entscheidet, sollte in jedem Fall
die Entwicklung der KI-VO im Blick behalten. Es gilt
abzuwägen, ob ein Festhalten an Altsystemen unter
Bestandsschutz langfristig Sinn macht oder ob nicht
doch eine frühzeitige Umstellung auf zukunftsfähige,
gesetzeskonforme Lösungen der bessere Weg ist.

Wie mit diesen Problemen umgegangen werden wird,
hängt auch von der weiteren Entwicklung ab. Wenn die
heutigen KI-Systeme bis 2O26 einen Stand erreichen,

Wo steht es?
Art. in KI-VO

8. Schnellcheck: Geltung der Betreiberpflichten im Einzelfall

KI-VO Fristen

1.8.2024
Inkrafttreten KI-VO

2.8.2025
Stichtag KI-Modelle

2.8.2027
Fristende KI-Modelle

2.2.2025
Stichtag KI-Kompetenz &
verbotene Praktiken

2.8.2026
Stichtag
Hochrisiko-KI-Systeme

2.8.2030
Fristende für
Behörden-KI

KI-Modelle mussen zum 2 8 2027
KI-VO-konform sein

Ab dem 2.2 2025 muss jeder Betreiber KI-Kompetenzen vermitteln

und es gelten die Verbote für bestimmte KI-Praktiken

0 0

Welche Pflichten hat der Betreiber nach der KI-VO> I  2 9

5 KI und Datenschutz

Entwickler von KI-Systemen verarbeiten enorme Datenmengen, um
KI-Modelle zu trainieren. Bei generativer KI, wie zum Beispiel Chatbots,
lernt das KI-Modell auf Grundlage öffentlich zugänglicher Daten aus dem
Internet. Dazu gehören unter anderem Einträge aus Wikipedia, Artikel von
Nachrichten-Websites und Beiträge aus Foren und Blogs sowie aus sozia-
len Netzwerken. Die KI-Trainingsdaten enthalten Angaben über bestimmte
Personen, sogenannte personenbezogene Daten.

Auch bei der Nutzung eines KI-Systems fallen personenbezogene Daten an.
Dies ist beispielsweise der Fall, wenn sich ein Nutzer bei einem KI-Chatbot
registriert oder wenn der Nutzer einen Befehl (Prompt) bei einem KI-Chat-
bot eingibt.

In beiden Fällen sind die Anforderungen des Datenschutzrechts zu beach-
ten. Im Folgenden werden nur die datenschutzrechtlichen Anforderungen
für den Einsatz eines KI Systems erläutert.

1. Anwendbarkeit des Daten-
schutzrechts

Die wichtigsten Pflichten zum Datenschutz sind in
der Datenschutz-Grundverordnung (DS-GVO) geregelt.
Die DS-GVO ist eine europäische Verordnung, die in
allen Mitgliedstaaten der Europäischen Union sowie
in Norwegen, Liechtenstein und Island unmittelbar
Anwendung findet. Die Verordnung enthält nur allge-
meine Regeln und somit keine spezifischen Pflichten
für die Datenverarbeitung im Zusammenhang mit
KI-Systemen. Auch die KI-Verordnung enthält keine
datenschutzrechtlichen Regelungen zum Einsatz von
KI-Systemen.

a) Personenbezogene Daten
Die Pflichten der DS-GVO sind immer dann zu beachten,
wenn personenbezogene Daten verarbeitet werden. Per-
sonenbezogene Daten sind alle Informationen, die sich
auf eine identifizierte oder identifizierbare natürliche
Person beziehen. Die geschützte Person wird im Daten-
schutzrecht als „betroffene Person" bezeichnet.

Bei der Nutzung eines KI-Systems fallen unter anderem
folgende personenbezogene Daten an:
• Benutzername und Passwort beim Login des

Nutzers;

• Chatverlauf/Historie;
• Daten über die Nutzung des KI-Systems (Datum, Uhr-
zeit, Standort, verwendetes Gerät, Lizenzangaben).

Darüber hinaus können Angaben zu einer Person im
Prompt enthalten sein.

€Z>BEISPIEL

Ein Mitarbeiter erhält eine englischsprachige E-Mail
und will diese durch das KI-System übersetzen
lassen. Er kopiert die E-Mail und fügt sie in das Ein-
gabefeld mit der Aufforderung ein, diesen Text in die
deutsche Sprache zu übersetzen.

In der Regel enthalten E-Mails eine Anrede und eine
Grußformel mit Signatur (Kontaktdaten des Ansprech-
partners zum Beispiel Telefonnummer, Anschrift). Bei

diesen Angaben handelt es sich um personenbezogene
Daten, da ein Rückschluss auf eine konkrete Person, den
Absender der E-Mail, möglich ist.

b) Besondere Kategorien personenbezogener
Daten
Die DS-GVO regelt, dass bestimmte Angaben zu einer
Person besonders schützenswert sind. Unter diese be-
sonderen Kategorien personenbezogener Daten fallen
solche Angaben, aus denen die rassische und ethnische
Herkunft, politische Meinungen, religiöse oder weit-
anschauliche Überzeugungen oder die Gewerkschafts-
zugehörigkeit hervorgehen. Darüber handelt es sich bei
genetischen Daten, biometrischen Daten, Gesundheits-
daten oder Daten zum Sexualleben oder der sexuellen
Orientierung einer natürlichen Person um besondere
Kategorien personenbezogener Daten. Bei der Verarbei-
tung besonderer Kategorien personenbezogener Daten
sind zusätzliche Pflichten zu beachten. So gelten bei-
spielsweise „strengere" Rechtsgrundlagen. Darüber hi-
naus sind diese Datenkategorien durch technische und
organisatorische Maßnahmen besonders zu schützen.

c) Datenschutzrechtliche Rolle der Akteure
Derjenige, der darüber entscheidet, für welche Zwecke
(warum) und mit welchen Mitteln (wie) personenbezo-
gene Daten verarbeitet werden, wird als Verantwort-
licher bezeichnet. Der Verantwortliche kann sowohl
eine juristische Person, zum Beispiel ein Unternehmen
oder eine Behörde, als auch eine natürliche Person sein.
In der Regel ist derjenige für die Datenverarbeitung
verantwortlich, der ein KI-System verwendet (Betreiber
im Sinne der KI-V0). Derjenige, der zum Beispiel das
KI-System als Cloud-Anwendung zur Verfügung stellt
(Anbieter im Sinne der KI-VO), wird als Auftragsver-
arbeiter bezeichnet. Der Auftragsverarbeiter verarbeitet
die personenbezogenen Daten im Auftrag des Verant-
wortlichen und muss dessen Weisungen ausführen. Die
DS-GVO regelt, dass Verantwortlicher und Auftragsver-
arbeiter einen Vertrag zur Auftragsbearbeitung ab-
schließen müssen. Der Vertrag zur Auftragsbearbeitung
muss unter anderem Angaben zur konkreten Daten-
verarbeitung, zu den betroffenen Kategorien perso-
nenbezogener Daten, zur Speicherdauer sowie zu den
technischen und organisatorischen Maßnahmen zum
Schutz der Daten enthalten.

KI und Datenschutz I  3 1

Anbieter Verantwortlicher für die
Entwicklung eines KI-Syslems

Betreiber = Verantwortlicher für die
Nutzung eines KI-Systems

- et> BEISPIELE

• Erstellen einer Geburtstagskarte mit KI-

generierten Bildern und Text;

• Urlaubsreise planen;
• Fremdsprache lernen.

Anbieter Auftragsverarbeitor für de
Bereitstellung eines KI-Systems

4  HINWEIS

1 Üblicherweise stellt der Auftragsverarbeiter einen

Vertrag zur Auftragsbearbeitung zur Verfügung. Der
Vertrag ist in der Regel auf der Webseite des Auf-
tragsverarbeiters veröffentlicht (gegebenenfalls im
Nutzerkonto) und kann elektronisch abgeschlossen
werden.

In besonderen Fällen können mehrere Akteure ge-
meinsam für die Datenverarbeitung verantwortlich
sein. Gemeinsam Verantwortliche müssen ebenfalls
einen Vertrag schließen, sogenannte Vereinbarung zur
gemeinen Verantwortlichkeit. In dieser Vereinbarung
ist zu regeln, wer welche Pflichten der DS-GVO erfüllt.
In welchen Fällen beim Einsatz von KI Systemen eine
gemeinsame Verantwortlichkeit vorliegt, ist derzeit
stark umstritten. Es spricht vieles dafür, dass jedenfalls
eine gemeinsame Verantwortlichkeit zwischen dem An-
bieter und dem Betreiber des KI-Systems besteht, wenn
Nutzungsdaten inklusive der Eingaben (Prompts) für das
weitere Training des KI-Systems genutzt werden.

4  HINWEIS
1  D i e  datenschutzrechtlichen Anforderungen im Falle
einer gemeinsamen Verantwortlichkeit sind komplex
und nach aktuellem Stand nicht immer leicht zu
erfüllen. Am einfachsten ist es, das Training im Be-
nutzerkonto zu deaktivieren.

Wo steht es?
Art.4, 9, 26, 28 DS-GVO

Nicht ausschließlich persönlich oder familiär ist
beispielsweise der Einsatz von KI-Systemen für Ver-
einszwecke oder wenn personenbezogene Daten aus
sozialen Netzwerken verarbeitet werden.

[ A  ACHTUNG

Nutzen Beschäftigte für dienstliche Zwecke KI-Sys-
teme, wird dies dem Arbeitgeber als Verantwort-
licher zugerechnet. Dies gilt auch dann, wenn
Beschäftigte ihre privaten Accounts für dienstliche
Zwecke nutzen.

Wo steht es
Art.2 DS-GVO

3. Rechtsgrundlagen

Die Datenverarbeitung ist nur zulässig, wenn die be-
troffene Person eine Einwilligung erteilt hat oder eine
gesetzliche Vorschrift die Datenverarbeitung erlaubt.
In den meisten Fällen entwickeln Unternehmen, Frei-
berufler oder Gewerbetreibende keine eigenen KI-Sys-
teme, sondern sie nutzen lediglich Anwendungen, die
von Anbietern wie zum Beispiel OpenAI zur Verfügung
gestellt werden. Das verwendete KI-System (zum Bei-
spiel ChatGPT) ist nur das Mittel zur Datenverarbeitung.
Entscheidend für die Frage, ob eine Rechtsgrundlage den
Einsatz von KI-Systemen erlaubt, ist der Zweck der Daten-
verarbeitung. Der Zweck entspricht im Wesentlichen
dem Ziel, das mit der Datenverarbeitung verfolgt wird.

2. Ausnahme für private Nutzung

Die datenschutzrechtlichen Pflichten gelten ausnahms-
weise nicht, wenn die Datenverarbeitung ausschließlich
für persönliche oder familiäre Tätigkeiten erfolgt.

BEISPIELE

Zweck:
Vertragsanbahnung/ Vertragsabschluss
• Kundenanfragen beantworten

3 2   K I   u n d   D a t e n b c h u t z

Vertragsanbahnung/ Vertragsabschluss
• Vertragsentwurf übersetzen; Kündigungsbedin-

gungen ermitteln; Verträge verwalten

Marketing:
• Werbetext, Video, Bilder entwerfen für Social

Media-Kampagne
Bewerbungsverfahren:
• Stellenausschreibung entwerfen
Reisekosten:
• Abrechnung erstellen
Dienstreise:
• Schnellste und günstigste Reiserouten ermitteln

Beim Einsatz von KI-Systemen durch nicht-öffentliche
Stellen kommen regelmäßig folgende Rechtsgrund-
lagen in Betracht:
• Einwilligung;
• Vertrag bzw. vorvertragliches Schuldverhältnis;
• Interessenabwägung.

a)  E i n w i l l i g u n g

Eine Einwilligung ist nur wirksam, wenn die betroffene
Person vorab, in informierter Weise und freiwillig in
die Datenverarbeitung eingewilligt hat. Vorab bedeutet,
dass die Datenverarbeitung nicht beginnen darf, bevor
die betroffene Person die Einwilligung erklärt hat. Nicht
ausreichend ist es, wenn die betroffene Person nach-
träglich die Datenverarbeitung genehmigt.

Informiert ist die Einwilligung, wenn die betroffene Person
in einer einfachen Sprache darauf hingewiesen wird:
•  f ü r  welche Zwecke die Datenverarbeitung erfolgt;
• welche Datenkategorien betroffen sind, insbeson-
dere ob besondere Kategorien personenbezogener
Daten verarbeitet werden;

•  w e r  an der Datenverarbeitung beteiligt ist (zum Bei-

spiel Auftragsverarbeiter);

•  w i e  lange die Daten verarbeitet werden und
•  o b  die Datenverarbeitung in einem Drittland (außer-

halb der EU) stattfindet.

9  HINWEIS
IAußerdem ist die betroffene Person darauf hinzu-

weisen, dass ihre Einwilligung jederzeit ohne Angabe
von Gründen widerrufen kann. Fehlt der Hinweis auf
das Widerrufsrecht, ist die Einwilligung unwirksam.

Freiwillig ist die Einwilligung, wenn die betroffene
Person eine echte Wahl hat. Das bedeutet, dass die be-
troffene Person die Datenverarbeitung auch ablehnen
kann, ohne dass dies negative Konsequenzen hat.

b )   V e r t r a g
Personenbezogene Daten dürfen verarbeitet werden,
wenn dies erforderlich ist, um vorvertragliche Maß-
nahmen durchzuführen oder einen Vertrag zu erfüllen.

Der Einsatz von KI-Systemen ist in der Regel keine ver-
tragliche Pflicht. Stattdessen werden die KI-Systeme
eingesetzt, um die Vertragsanbahnung oder die Erbrin-
gung der vertraglich geschuldeten Leistung effizienter
zu gestalten.

BEISPIEL

Ein Unternehmen erhält täglich mehrere Hundert
Kontaktanfragen per E-Mail. Bevor die E-Mails
beantwortet werden können, müssen sie an die
zuständigen Mitarbeiter in den Abteilungen Rech-
nungswesen, Reklamation und Kunden-Support
weitergeleitet werden. Es würde viel Zeit in Anspruch
nehmen, jede E-Mail zunächst inhaltlich zu sichten,
um sie dann dem jeweiligen Mitarbeiter zuzuordnen.
KI-Systeme können diese Aufgabe übernehmen. Das
Sortieren der E-Mails ist eine Datenverarbeitung, die
im Rahmen eines bestehenden Vertrages erfolgt.

c)  I n t e r e s s e n a b w ä g u n g
Die Datenverarbeitung ist auch rechtmäßig, wenn dies
erforderlich ist, um ein berechtigtes Interesse des Verant-
wortlichen oder eines Dritten zu wahren, sofern nicht die
Interessen oder Grundrechte und Grundfreiheiten der
betroffenen Person überwiegen. Die Interessen der be-
troffenen Personen überwiegen in der Regel dann, wenn:
• besondere Kategorien personenbezogener Daten ver-

arbeitet werden;

•  d i e  Datenverarbeitung aus Sicht der betroffenen

Person nicht erwartet wird oder derart komplex ist,
dass die Folgen für die betroffene Person nicht über-
schaubar;

• personenbezogene Daten besonders schützens-

werter Personengruppen, zum Beispiel Kinder ver-
arbeitet werden.

KI und Datenschutz  3 3

Lt ACHTUNG
Möchte der Verantwortliche personenbezogene
Daten auf Grundlage seiner berechtigten Interes-
sen verarbeiten, muss er dokumentieren, welches
konkrete Interesse er an der Datenverarbeitung hat
(zum Beispiel Werbung), warum es kein gleich ge-
eignetes, milderes Mittel gibt, um dieses Interesse
zu erreichen („Erforderlichkeit") und weshalb die
Interessen der betroffenen Personen nicht über-
wiegen.

Wo steht es?
Art.6, 9 DS-GVO

4. Dokumentationspflichten

Der Verantwortliche muss jederzeit nachweisen können,
dass er sämtliche Pflichten des Datenschutzrechts ein-
hält („Rechenschaftspflicht"). Die DS-GVO regelt zahl-
reiche Nachweispflichten.

Dazu gehören insbesondere:
• Nachweis einer Einwilligung, wenn die Datenver-

arbeitung darauf beruht,

• Verzeichnis von Verarbeitungstätigkeiten;
• Informationen zur Datenverarbeitung (Datenschutz-
bestimmungen/Datenschutzhinweise/Datenschutz-
erklärung);

• Angabe über technische und organisatorische Maß-

nahmen;

• Datenschutzfolgenabschätzung.

Der Einsatz eines KI-Systems muss demnach stets
dokumentiert werden.

et> BEISPIEL

Verzeichnis von Verarbeitungstätigkeiten (Auszug)

Zweck der Verarbei- Vertragsanbahnung/Vertragsab-
tung  s c h l u s s

Kundenanfragen beantworten

Rechtsgrundlage  A r t .  6 Abs.1 Buchstabe b DS-GVO;
Art. 6 Abs.1 Buchstabe c DS-GVO
in Verbindung mit Handelsgesetz-
buch, Abgabenordnung

Mittel der Verarbei- ChatGPT, CRM-Software
tung
Empfänger  O p e n A I ,  Microsoft usw.

Datenübermittlung USA
in ein Drittland
Speicherdauer  C h a t G P T  unverzüglich nach Be-
endigung des Chats; Historie und
Training wurden deaktiviert

Wo steht es?
Art.7,12 ff., 25, 30, 32, 35 DS-GVO

5. Allgemeine Informations-
pflicht

Verantwortliche müssen betroffene Personen über die
wesentlichen Umstände der Datenverarbeitung infor-
mieren. Die Informationspflicht ist leicht zu erfüllen,
indem zum Beispiel auf der Webseite des Unterneh-
mens in den Datenschutzbestimmungen Angaben zum
Einsatz des KI-Systems veröffentlicht werden. Da der
Einsatz von KI-Systemen kein Selbstzweck ist, sondern
es sich lediglich um ein Mittel zur Datenverarbeitung
handelt, können bereits vorhandene Datenschutzbe-
stimmungen leicht ergänzt werden.

4  HINWEIS
1  I m  Verzeichnis von Verarbeitungstätigkeiten sind
sämtliche Zwecke unter Angabe der Rechtsgrund-
lagen aufgeführt. Zu ergänzen sind unter „Mittel
der Verarbeitung" das eingesetzte KI-System zum
Beispiel ChatGPT sowie der Anbieter des KI-Systems
unter „Empfänger der Daten" zum Beispiel „OpenAI".

BEISPIEL
DATENSCHUTZBESTIMMUNGEN (AUSZUG)

Verantwortliche: Mustermann GmbH, Musterstraße
I, 12345 Musterstadt
Zweck der Datenverarbeitung: Kommunikation mit
Kunden und Interessenten

3 4  I  K I  und Datenschutz

Rechtsgrundlage: Art. 6 Absatz 1 Buchstabe b DS-GVO
Betroffene Kategorien: Kontaktdaten, Vertragsdaten,
Anfragen, [ggf ergänzen]
Datenempfänger: Microsoft (für die Anwendung von
Office-Diensten), OpenAl (für den Einsatz von KI-Sys-
temen), [ggf sonstige Software-Anbieter ergänzen]
[weitere Pflichtangaben einer Datenschutzbestim-
mang]

Wo steht es?
Art.12-14 DS-GVO

6. Drittstaatentransfer

Viele Entwickler von KI-Systemen haben ihren Sitz in
den USA oder einem anderen Staat außerhalb der EU
(Drittland). Die Pflichten der DS-GVO gelten jedoch auch
dann, wenn der Anbieter des KI-System keine Niederlas-
sung in der EU hat.

Selbst wenn ein US-amerikanischer KI-Anbieter auch
in der Europäischen Union eine Niederlassung hat,
bedeutet das keinesfalls, dass die Datenverarbeitung
ausschließlich in der EU stattfindet. Oftmals stehen die
KI-Anwendungen als Cloud-Dienst zur Verfügung. Die
Server des Cloud-Dienstes können weltweit betrieben
werden, sodass häufig personenbezogene Daten (auch)
außerhalb der EU verarbeitet werden.

--> HINWEIS

Anbieter von KI-Systemen müssen darüber infor-
mieren, ob personenbezogene Daten in einem Dritt-
land verarbeitet werden. Angaben hierzu sind auch
im Vertrag zur Auftragsbearbeitung enthalten.

Der Datentransfer in ein Drittland ist nur zulässig, wenn
weitere Pflichten der DS-GVO erfüllt werden. Der Ver-
antwortliche muss dafür sorgen, dass in dem Drittland
ein angemessenes Schutzniveau besteht. Die EU-Kom-
mission erleichtert Verantwortlichen diese Prüfung,
indem sie untersucht, ob in einem Drittstaat ein solches

Angemessenheitsniveau besteht. Ist dies der Fall, erfolgt
für ein Drittland ein sogenannter Angemessenheits-
beschluss. Das bedeutet, dass Verantwortliche Daten
in einem Drittstaat übermitteln können, ohne weitere
datenschutzrechtliche Pflichten beachten zu müssen.
Die für folgenden Drittländer liegt ein Angemessen-
heitsbeschluss vor: Andorra, Argentinien, Kanada,
Färöer-Inseln, Guernsey, Israel, Isle of Man, Japan, Jersey,
Neuseeland, Südkorea, Schweiz, Großbritannien, USA
und Uruguay.

— A ACHTUNG

Handelt es sich bei dem Anbieter des KI-Systems
um ein US-amerikanisches Unternehmen, müssen
Verantwortliche prüfen, ob dieses Unternehmen
nach dem sogenannten „Data Privacy Framework",
das heißt dem Angemessenheitsbeschluss für die
USA, zertifiziert ist. Das US-Handelsministerium
veröffentlicht auf der Webseite unter https://www.
dataprivacyframework.govi eine Liste, mit allen
zertifizierten US-amerikanischen Unternehmen.

Liegt kein Angemessenheitsbeschluss vor oder handelt
es sich im Falle der USA um ein Unternehmen, welches
nicht nach dem Data Privacy Framework zertifiziert ist,
können Unternehmen sogenannte Standardvertrags-
klauseln (SCCs) abschließen. Bei den SCC handelt es sich
um Vertragsmuster, die ebenfalls von der EU-Kommis-
sion zur Verfügung gestellt werden (https://commission.
europa.euipublicationsistandard-contractual-clauses-
international-transfers_de).

—> HINWEIS
I I n  der Regel stellen die Anbieter des KI-Systems die
SCC im Rahmen des Vertrags zur Auftragsverarbei-
tung zur Verfügung. Die Vertragsdokumente sind
meinst online auf der jeweiligen Website des Anbie-
ters abrufbar.

Wo steht es?
Art.4.4ff. DS-GVO

KI und Datenschutz I  3 5

6 KI und Arbeitsrecht

KI hält immer mehr Einzug in die Personalabteilungen von Unternehmen.
Egal ob bei der Auswahl von Bewerbern, der Verwaltung von Mitarbeiter-
daten oder der Planung von Weiterbildungen — überall kommt KI-gestütz-
te Software zum Einsatz, die Entscheidungen unterstützen oder sogar
selbstständig treffen soll. In diesem Kapitel wird erklärt, was Unternehmen
und Behörden dabei zu beachten haben.

1. KI in der Arbeitswelt

Doch was genau steckt hinter diesen Programmen? Im
Grunde genommen handelt es sich um sehr leistungs-
fähige Software, die große Mengen an Daten analysieren
kann. Dazu zählen beispielsweise die Lebensläufe und
Zeugnisse von Bewerbern oder die Leistungsdaten von
Mitarbeitern. Aus diesen Informationen versucht die KI,
Muster zu erkennen und daraus Vorhersagen abzuleiten.
So könnte sie etwa berechnen, welcher Bewerber am
besten zu einer ausgeschriebenen Stelle passt oder wel-
cher Mitarbeiter für eine Beförderung in Frage kommt.

Das klingt zunächst einmal praktisch und zeitsparend.
Doch der Einsatz von KI birgt auch Risiken. Zum einen
besteht die Gefahr, dass die Programme diskriminie-
rend entscheiden, indem sie beispielsweise bestimmte
Personengruppen benachteiligen. Zum anderen können
falsche oder unvollständige Daten zu Fehleinschät-
zungen führen. Nicht zuletzt gibt es Bedenken hin-
sichtlich des Datenschutzes und der Privatsphäre der
Betroffenen.

Um diese Risiken zu minimieren, hat der Gesetzgeber
strenge Regeln für den Einsatz von KI aufgestellt. Unter-
nehmen müssen eine Reihe von Auflagen erfüllen,
bevor sie solche Programme einsetzen dürfen:
• wenige Verbote beachten;
• viele KI-Systeme auf der Arbeit sind erlaubt, aber

hochriskant;

• Pflichten aus dem Arbeitsrecht gelten auch für KI-

Systeme.

2. KI-Verbote: Was Arbeitgeber
nicht dürfen

Auch wenn viele KI-Anwendungen im Arbeitskontext
grundsätzlich erlaubt sind, gibt es einige Praktiken, die
der Gesetzgeber ausdrücklich untersagt hat. Dahinter
steckt die Überlegung, dass bestimmte KI-Systeme
eine inakzeptable Gefahr für die Sicherheit, die Grund-
rechte und die Gesundheit von Beschäftigten darstellen
können.

BEISPIEL: MANIPULATION DURCH UNTER-
SCHWELLIGE BEEINFLUSSUNG

Verboten wäre es, wenn eine Software zur Verwal-
tung eines Warenlagers die Mitarbeiter durch ein
Punktesystem oder Ranglisten dazu bringen würde,
immer schneller zu arbeiten  a u c h  wenn dabei
Sicherheitsvorschriften missachtet werden.

Solche intransparenten „Gamification"-Elemente, die
durch versteckte Anreize oder unterschwellige Sank-
tionsandrohungen einen gefährdenden Leistungsdruck
erzeugen, sind unzulässig. Es geht dabei um intrans-
parente Fremdsteuerung mit Gefährdungspotenzial.
Transparente Systeme, die mangels Sanktionsdruck
nicht zur Gefahr des Verstoßes gegen Sicherheitsvor-
schriften führen, wären hingegen nicht verboten.

BEISPIEL: GEZIELTE BENACHTEILIGUNG
SCHUTZBEDÜRFTIGER GRUPPEN

Ein weiteres Verbot betrifft KI-Systeme, die gezielt
schutzbedürftige Gruppen wie Ältere, Menschen
mit Behinderungen oder Geringverdiener benach-
teiligen. Ein Beispiel wäre eine Schichtplanungs-KI,
die Alleinerziehende mit Betreuungspflichten für
weniger flexibel und daher ineffizient halten würde
und deshalb seltener für Dienste einplant. Erlaubt
bleiben hingegen KI-Anwendungen, die solche Be-
schäftigten gezielt unterstützen, indem sie etwa
flexible Arbeitszeitfenster als freiwillige Optionen
anbieten.

BEISPIEL: EMOTIONSERKENNUNG AM
ARBEITSPLATZ

Ausdrücklich verboten ist auch der Einsatz von
Emotionserkennungssystemen am Arbeitsplatz,
sofern dies nicht ausnahmsweise aus zwingenden
medizinischen oder Sicherheitsgründen erforderlich
ist. Arbeitgeber dürfen also nicht die Mimik, Stimme
oder biometrischen Daten ihrer Beschäftigten durch
KI auswerten lassen, um deren Motivation, Zufrie-
denheit oder Loyalität zu überwachen.

KI und Arbeitsrecht I  3 7

Unzulässig wären etwa Gesichtserkennungssysteme, die
bei Videokonferenzen die Aufmerksamkeit der Teil-
nehmer messen und Warnhinweise ausgeben, wenn
jemand abgelenkt oder genervt wirkt. Nur in Aus-
nahmefällen, etwa bei Hochrisikotätigkeiten wie dem
Steuern von Fahrzeugen, können solche Systeme als
Müdigkeitswarnung gerechtfertigt sein.

3. Hochriskante KI am
Arbeitsplatz

Der Anwendungsbereich für Hochrisiko-KI im Beschäf-
tigungskontext ist in der Verordnung sehr weit gefasst.
Er umfasst die Bereiche „Beschäftigung", „Personalma-
nagement" und „Zugang zur Selbstständigkeit". Aller-
dings wird nicht näher definiert, was genau unter diese
Begriffe fällt.

a) Erfasste Sektoren
Der Begriff „Beschäftigung" ist im EU-Recht weit zu
verstehen. Er bezieht sich auf alle Beschäftigungsver-
hältnisse, bei denen eine Person weisungsgebunden für
eine andere tätig ist.

Mit „Zugang zur Selbstständigkeit" ist wiederum nicht
jede Form von Selbstständigkeit gemeint. Im Gegensatz
zur Gründung und zum Aufbau eines Unternehmens,
wird hiermit die Möglichkeit beschrieben, über digitale
Plattformen als solo-selbstständiger Auftragnehmer
tätig zu werden, etwa als Fahrdienstleister, Essensliefer-
dienste oder Freelancer.

Der Begriff „Personalmanagement" bezieht sich auf den
Einsatz von Hochrisiko-KI in Prozessen der Personal-
wirtschaft, die noch keinen direkten Bezug zu einzelnen
Beschäftigten haben, wie etwa die strategische Personal-
planung.

b) Erfasste KI-Praktiken
Innerhalb dieser Sektoren gelten nach dem Gesetz be-
stimmte Tätigkeiten als hochriskant:
• Einstellung und Auswahl von Bewerbern, beispiels-
weise durch die automatisierte Analyse von Bewer-
bungsunterlagen.

• KI-Einsätze, welche Entscheidungen über die Bedin-

gungen der Beschäftigung betreffen.

4  HINWEIS
I D e r  Begriff der Bedingung der Beschäftigung ist sehr
weit geraten. Erfasst ist jede KI, die zum Treffen von
Arbeitgeberentscheidungen eingesetzt wird, die sich
auf die Arbeitsbedingungen auswirkt. Hier besteht
die Gefahr, dass die Grenze zwischen prägenden KI-
Urteilen und neutralen Informationen zur Unterstüt-
zung menschlicher Entscheidungen verschwimmt.

Problem: Besonders schwierig wird die Abgrenzung in
Fällen, in denen KI-Systeme zur Unterstützung unter-
nehmerischer Entscheidungen eingesetzt werden, die
sich typischerweise auf die Beschäftigten als Kollektiv
auswirken, auch wenn sich die konkreten Folgen für die
Arbeitsbedingungen noch nicht realisiert haben:
•  E i n  KI-System wertet die Nutzung von Büroräumen
und Arbeitsplätzen aus und schlägt vor, bestimmte
Standorte aufzugeben und die Belegschaft zu kon-
zentrieren. Auch hier wären die Beschäftigten als
Kollektiv betroffen, selbst wenn die individuellen
Konsequenzen noch unklar sind.

•  E i n e  KI wertet die Nutzung des Fuhrparks aus und
rät, bestimmte Fahrzeugtypen abzuschaffen und
dafür mehr Poolfahrzeuge anzuschaffen. Diese Ent-
scheidung beträfe die auf diese Fahrzeuge angewie-
senen Beschäftigten kollektiv, auch wenn Art und
Ausmaß noch unklar sind.

Um mehr Rechtssicherheit zu schaffen, werden schon
jetzt Leitlinien der EU-Kommission geschaffen, welche
näher beschreiben sollen, welche KI-Systeme als hoch-
riskant gelten sollen. Auf diese Weise könnte klargestellt
werden, wann genau ein KI-System als Hochrisiko-An-
wendung im Beschäftigungskontext gilt und wann nicht.

4. ,,Normales" Arbeitsrecht

Grundsätzlich bleiben die bestehenden Pflichten und
Rechte aus dem deutschen Arbeitsrecht auch bei der An-
wendung von KI-Systemen im Beschäftigungskontext
bestehen. Dennoch ergeben sich durch die KI-VO einige
neue Aspekte und Herausforderungen.

a) Arbeitgeberweisungen
Ein zentraler Punkt ist das Direktionsrecht des Arbeitge-
bers. Dieses erlaubt es ihm, Beschäftigte nach billigem

3 8  1  K I  und Arbeitsrecht

Ermessen zur Nutzung von KI-Systemen am Arbeits-
platz anzuweisen. Grundsätzlich muss sich ein Arbeit-
geber nur den KI-Einsatz zurechnen lassen, der unter
seiner Autorität erfolgt. Das bedeutet, dass er für das
Handeln seiner Mitarbeiter im Rahmen der von ihm
vorgegebenen Richtlinien und Weisungen haftet. Wenn
ein Mitarbeiter jedoch gegen diese Richtlinien verstößt
und KI-Systeme eigenständig oder abweichend von den
Vorgaben einsetzt, kann dies das Haftungsrisiko für den
Arbeitgeber senken.

4  HINWEIS

Es ist daher wichtig, dass der Arbeitgeber eindeutige
Regeln aufstellt, wer in welchem Umfang und zu
welchem Zweck KI-Systeme im Unternehmen nutzen
darf. Dabei sollte genau festgelegt werden, welche
Anwendungen erlaubt sind, welche Daten verarbeitet
werden dürfen und welche Sicherheitsmaßnahmen
zu ergreifen sind. Auch die Verantwortlichkeiten und
Aufsichtspflichten sollten klar verteilt werden.

Diskriminierungsvorwürfe zu entkräften. Allerdings
birgt gerade der Einsatz von generativen KI-Systemen
die Gefahr von Verstößen gegen das Allgemeine Gleich-
behandlungsgesetz, wenn der Arbeitgeber keinen Ein-
blick in die Entscheidungsfindung der KI hat.

c) Betriebsrat bestimmt mit
Bei der Einführung von KI-Systemen im Unternehmen
darf der Arbeitgeber nicht eigenmächtig handeln, son-
dern muss die Mitbestimmungsrechte des Betriebsrats
beachten. Das heißt, der Betriebsrat hat ein Mitsprache-
recht und muss in bestimmten Fällen zustimmen, bevor
der Arbeitgeber KI-Systeme einsetzen darf.

Besonders wichtig ist die Mitbestimmung, wenn die
KI dazu genutzt werden kann, das Verhalten oder die
Leistung der Arbeitnehmer zu überwachen. Das ist im
Betriebsverfassungsgesetz klar geregelt. Dort steht, dass
der Betriebsrat zwingend mitbestimmen muss, wenn
technische Einrichtungen eingeführt werden, die zur
Überwachung geeignet sind.

Verstößt ein Mitarbeiter gegen diese Richtlinien und
setzt KI-Systeme eigenmächtig oder entgegen der Vor-
gaben ein, kann sich der Arbeitgeber darauf berufen,
dass dieser Einsatz weisungswidrig erfolgte. Er kann
argumentieren, dass er für dieses Verhalten nicht haftet,
da es außerhalb seiner Autorität und Kontrolle geschah.

[

A  ACHTUNG
Dabei spielt es keine Rolle, ob die KI selbst Daten
über die Arbeitnehmer sammelt oder nur vor-
handene Daten auswertet. In beiden Fällen hat der
Betriebsrat ein Mitbestimmungsrecht.

b) KI Im Bewerbungsprozess
Besondere Vorsicht ist beim Einsatz von KI im Bewer-
bungsprozess geboten. Hier können die Dokumen-
tationspflichten für Hochrisiko-KI-Systeme helfen,

Wo steht es?
Art. 5, 6 KI-VO in Verbindung mit Anhang III Ziffer 4;
§§ 87, 90 BetrVG

KI und Arbeitsrecht I  3 9

7 KI und Verbraucher-

schutzrecht

Auch als Verbraucher hat man nach der KI-VO bestimmte Rechte im
Umgang mit künstlicher Intelligenz. Allerdings unterscheidet sich die Art
dieser Rechte von denen, die man vielleicht aus der Datenschutz-Grund-
verordnung kennt. In diesem Kapitel werden Verbraucher, Unternehmen
und Behörden über die entsprechenden Pflichten und Rechte informiert.

1. Verbraucherrechte

Die KI-VO ist in erster Linie eine Produktregulierung.
Das bedeutet, sie stellt Anforderungen an die Hersteller
und Betreiber von KI-Systemen, um die Sicherheit und
Grundrechte der Nutzer zu schützen. Sie gibt dem Ver-
braucher aber keine umfassenden Beschwerderechte.
Das heißt, es bestehen nach der KI-VO keine festen
Fristen oder ein Anspruch darauf, dass die Marktüber-
wachungsbehörden bei Rechtsverstößen tätig werden
müssen.

2. Recht auf Beschwerde

Wenn man aber den Eindruck hat, dass ein KI-System
nicht den Anforderungen an Transparenz, Robustheit
oder menschliche Aufsicht genügt, kann man dies der
Behörde melden, welche die Beschwerde prüft und gege-
benenfalls Marktüberwachungsmaßnahmen einleitet,
etwa Kontrollen vor Ort, Informationsanforderungen
an den Hersteller oder sogar Verkaufsverbote.

-*HINWEIS
IAllerdings hat man nach der KI-VO keinen Anspruch
darauf, dass die Behörde eine bestimmte Maßnahme
innerhalb einer Frist ergreift.

3. Recht auf Erläuterung

Das Recht auf Erläuterung nach der KI-VO ist besonders
relevant, wenn man von Entscheidungen betroffen ist,
an denen Hochrisiko-KI-Systeme beteiligt waren. Das
können zum Beispiel Entscheidungen über Kreditver-
gaben, Versicherungsangebote, die Zuteilung von Sozial-
leistungen oder auch medizinische Diagnosen sein.
Wenn man hier den Eindruck hat, dass die Entscheidung
nicht nachvollziehbar ist oder einen unverhältnismäßig
hart trifft, kann man eine Erklärung verlangen. Diese
muss verständlich darlegen, welche Rolle die KI gespielt
hat und auf welchen Kriterien die Entscheidung beruht.
So kann man besser einschätzen, ob die eigenen Rechte
gewahrt wurden und gegebenenfalls weitere Schritte
einleiten.

r

A  ACHTUNG
Eine Grenze findet dieses Recht aber dort, wo es
mit anderen Gesetzen oder geschützten Interessen
kollidiert, etwa mit Geschäftsgeheimnissen oder
der nationalen Sicherheit.

4. Whistleblowing

Die KI-Verordnung sieht auch einen Hinweisgeberschutz
vor. Das ist relevant für alle, die im Umfeld von KI-
Systemen arbeiten und dort Verstöße gegen die KI-VO
bemerken. Das können Beschäftigte von Herstellern
oder Betreibern sein, aber auch Externe wie beauftragte
Prüfer oder Zertifizierungsstellen. Sie können Miss-
stände melden, ohne berufliche Nachteile oder andere
Repressalien befürchten zu müssen. Im Detail sind
solche Vorgaben in der Whistleblower-Richtlinie und im
Hinweisgeberschutzgesetz geregelt.

5. Transparenz

Neben diesen Rechten gibt es auch Pflichten für Unter-
nehmen beim Einsatz von KI-Systemen, die mit Ver-
brauchern in Kontakt treten. Denn jede natürliche
Person muss darüber informiert werden, wenn sie mit
KI-Systemen interagiert, die Texte, Bilder, Audio- oder
Videoinhalte generiert. Die Art und Weise, wie diese
Informationen bereitgestellt werden, muss dabei leicht
verständlich und zugänglich sein. Es reicht also nicht aus,
die Verwendung von KI irgendwo im Kleingedruckten
zu verstecken. Stattdessen müssen die Informationen so
aufbereitet werden, dass auch technisch weniger ver-
sierte Nutzer sie wahrnehmen und verstehen können.

BEISPIEL

Ein Beispiel für die Auswirkungen dieser Vorgabe im
Verbraucherkontext sind KI-generierte Produktbewer-
tungen, Empfehlungen, oder Produktdarstellungen
in Online-Shops oder sozialen Medien. Hier muss für
Verbraucher auf einen Blick ersichtlich sein, dass die
Produktbewertung oder Fotos nicht von anderen Nut-
zern stammen, sondern maschinell erzeugt wurden.

Wo steht es?
Art. 50, 85, 86 KI-VO

KI und Verbraucherschutzrecht 1  4 1

8 KI und Urheberrecht

Wer ein literarisches oder künstlerisches Werk schöpft, hat ein Interesse
daran, selbst über dessen Verwertung zu entscheiden. Das Urheberrecht
schützt das geistige Eigentum des Urhebers deshalb vor einer unkon-
trollierten Verwertung durch Dritte. Zum Training einer künstlichen
Intelligenz sind allerdings Unmengen an Daten erforderlich, die zum
Teil urheberrechtlich geschützt sind. An ihnen lernt das System die wahr-
scheinlichste Wortfolge oder Pixelzusammensetzung. Das Spannungsver-
hältnis zwischen dem Urheberrecht und dem KI-Einsatz liegt damit auf
der Hand. Nicht umsonst klagen in den USA zahlreiche Urheber gegen
KI-Anbieter wegen der Verletzung ihrer Rechte durch das Training und
den Einsatz der Systeme. Wie das deutsche Recht mit diesem Spannungs-
verhältnis umgeht und was Anwender deshalb beachten müssen, wird in
diesem Kapitel beleuchtet.

1. Schutz der Eingabe

Eine urheberrechtliche Betrachtung der KI-Einsatzes
muss zwischen der Eingabe und der Ausgabe differen-
zieren. Die Eingabe ist die Anfrage (engl. Prompt) des
Anwenders. Hier stellen sich vor allem zwei Fragen:

a) Darf ich urheberrechtlich geschützte Inhalte
in das KI-System eingeben?
Schon wer Kapitel aus Harry Potter Werken in einen
Chatbot eingibt und sich zusammenfassen lassen
möchte, muss das Urheberrecht beachten. Denn grund-
sätzlich schützt das Urheberrecht vor jeder Form der
Vervielfältigung eines Werkes. Die Kopie eines urheber-
rechtlich geschützten Werkes ist daher nur mit Er-
laubnis des jeweiligen Rechtsinhabers zulässig. Das gilt
unabhängig davon, ob es sich um eine vorübergehende
oder dauerhafte Kopie handelt und in welchem Ver-
fahren oder in welcher Zahl sie erstellt wurde. Damit ist
jedenfalls im Ausgangspunkt bereits das Kopieren und
Einfügen eines urheberrechtlich geschützten Textes
eine Urheberrechtsverletzung.

Aber: Dass dieser Zustand nicht haltbar wäre, zeigt
sich allerdings bereits am Beispiel der Suchmaschinen:
Sollen sich Bürger wirklich der Gefahr von Schaden-
ersatzforderungen aussetzen, wenn sie im Internet eine
bestimmte Stelle ihres Lieblingsromans suchen? Das
Urheberrecht hilft hier aus und bestimmt, dass vorüber-
gehende Vervielfältigungen ausnahmsweise zulässig
sind, wenn sie flüchtig sind und keine eigenständige
wirtschaftliche Bedeutung haben. Die Suchmaschinen-
anfrage ist damit regelmäßig aus dem Schneider.

Was gilt nun für KI-Systeme? Viele Systeme behandeln
die Anfragen der Nutzer nicht wie eine Suchmaschinen-
Anfrage. Vielmehr nutzt das System die Inhalte der
Anfrage, um seine Ergebnisse weiter zu verbessern. Die
eingegebenen Daten landen also in der KI und dienen
abstrakt als Grundlage weiterer Berechnungen. Flüchtig
ist diese Form der Verwertung nicht.

-> HINWEIS:
IZumindest wenn das eingesetzte System die Eingaben
verwendet, um seine Prozesse zu verbessern, sollten
Anwender deshalb davon absehen, ihren Anfragen ur-
heberrechtlich geschützte Inhalte zugrunde zu legen.

Wo steht es?
§44a UrhG

b) Sind meine Eingaben urheberrechtlich
geschützt?
Die Potenziale eines KI-Systems sind stark abhängig von
den Fähigkeiten des jeweiligen Nutzers. Wie ein hoch-
gezüchtetes Turnierpferd benötigt auch das System zur
vollen Leistungsfähigkeit einen fähigen Menschen, der
es kontrolliert. Dazu zählt neben einem Verständnis für
die Gefahren des KI-Einsatzes die Fähigkeit, dem System
eindeutig zu vermitteln, was von ihm gewünscht ist.

Eine gut formulierte Anfrage, mit der die Potenziale
eines KI-Systems ausgereizt werden können, ist daher
von einigem Wert. Online werden entsprechende For-
mulierungen bereits gehandelt und mit den „Prompt
Engineers", also den „Anfrageingenieuren" hat sich
bereits eine neue Berufsgruppe gebildet. Vor diesem
Hintergrund stellt sich die Frage, ob die Anfragen selbst
urheberrechtlichen Schutz genießen können.

Das Urheberrecht verlangt zur Entfaltung seines
Schutzes eine gewisse Schöpfungshöhe. Grundsätzlich
können ihm zwar auch kurze Texte unterfallen. Erfor-
derlich ist aber ein gewisses Maß an Individualität und
Kreativität.

-> HINWEIS

Einfache Prompts werden diese Schöpfungshöhe re-
gelmäßig nicht erreichen. Denkbar ist ein Schutz erst
bei umfangreichen Anfragen, in denen der Anwender
dem System gestalterische Vorgaben macht, die Aus-
fluss seiner Kreativität sind.

2. Schutz der Ausgabe

Die Ergebnisse der KI-Systeme werden als Ausgabe (engl.
Output) bezeichnet. Auch in Bezug auf KI-generierte In-
halte stellt sich die Frage urheberrechtlichen Schutzes.
Dabei sind wiederum zwei Aspekte klärungsbedürftig:

Kommt den KI-generierten Inhalten urheber-
rechtlicher Schutz zu?
Zunächst stellt sich die Frage, ob KI-generierten Inhalten
urheberrechtlicher Schutz zukommt. Das ist regelmäßig

KI und Urheberrecht I  4 3

nicht der Fall. Im Fokus des Urheberrechts steht der
Urheber, also der Schöpfer des Werkes. Dieser kann
nur ein Mensch sein. Erst wenn ein Mensch dem KI-
System so konkrete Vorgaben macht, dass es nur noch
als Werkzeug zur Herstellung des Werkes betrachtet
werden kann, ist ein Urheberrechtsschutz ausnahms-
weise denkbar. Angesichts der Fähigkeiten moderner
KI-Systeme lässt sich aber daran zweifeln, ob sich die
Kreativität des Menschen überhaupt noch gegen die Be-
rechnungen des Systems durchzusetzen vermag.

b) Kann ich durch die Verbreitung KI-generier-
ter Inhalte das Urheberrecht Dritter verletzen?
Von größerer Relevanz ist die Frage, ob der Anwender
eines KI-Systems Gefahr läuft, Urheberrechte zu ver-
letzen, wenn er KI-generierte Inhalte verwertet. Er-
freulich dürfte zunächst die Nachricht sein, dass der
Stil eines Künstlers keinen urheberrechtlichen Schutz
genießt.

BEISPIEL

Ein KI-generiertes Bild im Stil von Picasso und ein
Text im Stil von Goethe verletzen daher nicht das
Urheberrecht.

Problematisch ist dagegen, dass KI-Systeme bisweilen
große Teile ihrer Trainingsdaten zusammenhängend
wiedergeben. Forscher haben nachgewiesen, dass einige
KI-Modelle bei entsprechender Anfrage ganze Kapitel
urheberrechtlich geschützter Bücher wiedergeben.
Außerdem kommt es vor, dass größere Teile eines KI-
generierten Bildes bekannten Werken entspringen, die
urheberrechtlichen Schutz genießen.

BEISPIEL

Ein KI-generiertes Bild eines Comic-Superhelden im
Stil von Spiderman könnte urheberrechtlich proble-
matisch sein. Noch komplexer wird es, wenn ein Bild
eines Piraten im Stile der Filmreihe Fluch der Karibik
in der Anmutung des Schauspielers Ionny Depp
generiert werden soll: Hier treten etwa marken- und
persönlichkeitsrechtliche Probleme hinzu.

HINWEIS

Solange der KI-generierte Inhalt noch als Vervielfäl-
tigung oder Bearbeitung des geschützten Werkes be-
trachtet werden muss, ist das Urheberrecht betroffen
Der KI-Anwender darf das Ergebnis der KI deshalb nu;
mit Zustimmung des Rechtsinhabers verwerten.

Die Grenze zwischen Urheberrechtsschutz und freier
Verwertbarkeit liegt dort, wo das geschützte Werk im
KI-generierten Ergebnis wiedererkennbar ist. Ist eine
Erkennbarkeit ausgeschlossen, etwa weil das Werk stark
verfremdet wurde oder weil es nur einen vernachläs-
sigbaren Teil des Gesamtergebnisses bildet, kann der
KI-Anwender den KI-generierten Inhalt frei verwerten.
Diese Grenze gilt auch, wenn in einem KI-generierten
Bild urheberrechtsrelevante Teile enthalten sind, das
Bild im Anschluss aber von einem Menschen weiter
bearbeitet wird. Geht im Zuge der weiteren Bearbeitung
die Erkennbarkeit des geschützten Werkes verloren, darf
das Bild frei verwertet werden.

—A ACHTUNG

Haftungsrisiko!
In der Praxis kann es schwierig sein, geschützte
Werke in der Ausgabe eines KI-Systems als solche
zu erkennen. KI-Anbieter nutzen zwar mittler-
weile vermehrt Techniken, um die zusammen-
hängende Ausgabe von Trainingsmaterialien zu
verhindern. Mit Sicherheit kann die Ausgabe eines
urheberrechtlich geschützten Ergebnisses aber
nicht ausgeschlossen werden. Wer kennt schon alle
Designer-Möbel so genau, um zu entscheiden, ob
ein vom KI-System generiertes Möbelstück diesem
nachgeahmt worden ist? Wahrscheinlich ist das,
denn gerade anhand von im Internet zugänglicher
Bilder von Möbelstücken sind die KI-Modelle trai-
niert worden.

HINWEIS

KI-Anwender sollten die Ergebnisse der KI daher stets
kritisch überprüfen und bei Zweifeln von der Ver-
wertung des Inhalts absehen. Hilfreich kann es sein,
ein Bild nicht in einem Prompt generieren zu lassen,
sondern verschiedene Aspekte des gewünschten

4 4   E l  umd

I B i l d e s  in mehreren Prompts hinzuzufügen, um die
Wahrscheinlichkeit einer Wiedergabe zusammen-
hängender Trainingsinhalte zu verringern.

würden eine solche Erklärung nicht als Widerspruch er-
kennen. Die Wahrscheinlichkeit wäre groß, dass sie den
Widerspruch selbst zu Trainingszwecken verwenden.

3. Wie schütze ich meine
Werke vor einer Verwertung im
KI-System?

KI-Anbieter sind auf große Mengen an Daten ange-
wiesen, um ihre Systeme zu trainieren. Wie gezeigt,
kann es vorkommen, dass die Trainingsinhalte in den
Ergebnissen des Systems auftauchen. Aber auch sonst
haben Autoren, Texter, Künstler und Grafiker ein Inte-
resse daran, ihre Daten nicht für das Training einer KI
zur Verfügung zu stellen. Wie aber können sie sich davor
schützen? Das Urheberrecht lässt eine Vervielfältigung
geschützter Werke im Rahmen des Trainings einer
künstlichen Intelligenz grundsätzlich zu, wenn diese
frei zugänglich sind. Es sieht aber auch die Möglichkeit
für Rechtsinhaber vor, der Vervielfältigung im Einzel-
fall zu widersprechen. Erforderlich ist dafür ein Wider-
spruch in maschinenlesbarer Form. Eine Erklärung in
natürlicher Sprache in den AGB oder im Impressum
des Internetauftritts genügt daher nicht. Die Systeme

HINWEIS
1  D e r  Widerspruch kann etwa im Code einer Internet-
seite oder in den Metadaten eines Bildes hinterlegt
sein. Damit die Systeme den Widerspruch als solchen
erkennen können, muss ihm eine eindeutige Hand-
lungsanweisung zugrunde liegen.

Das erfordert eine technische Standardformulierung,
die für die Systeme verständlich ist. Leider hat sich ein
solcher Standard bisher nicht etabliert. Als erster An-
satz für einen standardisierten Widerspruch wurde ein
sogenanntes TDM Reservation Protocol entwickelt, das
auf der Internetseite des World Wide Web Consortiums
abrufbar ist.

Wo steht es?
Art. 50, 85, 86 KI-VO

KI und Urheberrecht I  4 5

Glossar

Anbieter: Jeder, der ein KI-System oder ein KI-Modell ent-
wickelt oder entwickeln lässt (Art.3 Nr.3 KI-VO).

Auftragsverarbeiter: Jeder, der personenbezogene Daten
im Auftrag einer anderen Person verarbeitet (Art.4 Nr.8
DS-GVO).

Betreiber: Jeder, der ein KI-System in eigener beruflicher
Verantwortung verwendet (Art.3 Nr.4 KI-VO).

Betroffene Person: Jede natürliche Person, auf die sich per-
sonenbezogene Daten beziehen (vgl. Art.4 Nr.i DS-GVO).

BetrVG: Betriebsverfassungsgesetz; Gesetz zur Regelung
der Zusammenarbeit von Arbeitgeber und Betriebsrat.

Bias: Gewichtung, die in einem KI-System die Bewertung
von Eingabedaten bestimmt.

Chatbot: Computerprogramm, das menschenähnliche Ge-
spräche führen kann.

ChatGPT: Fortschrittlicher Chatbot, der auf KI-Technolo-
gien basiert.

Deepfakes: Bild-, Ton- oder Videoinhalte, die von einer KI
erzeugt oder manipuliert wurden und wirklichen Per-
sonen, Gegenständen, Orten, Einrichtungen oder Ereig-
nissen in einer Weise ähneln, dass sie fälschlicherweise als
echt oder wahrheitsgemäß erscheinen (Art.3 Nr. 6o KI-VO).

DS-GVO: Datenschutz-Grundverordnung; Gesetz zum
Schutz natürlicher Personen bei der Verarbeitung perso-
nenbezogener Daten.

Gemeinsam Verantwortliche: Zwei oder mehr Personen,
die gemeinsam über die Zwecke und Mittel einer Verarbei-
tung personenbezogener Daten entscheiden (vgl. Art.4 Nr.7
DS-GVO).

Generative KI: KI-System, das Audio-, Bild-, Video- oder
Textinhalte erzeugen kann.

GPAI-System: KI-System mit allgemeinem Verwendungs-
zweck; KI-System, das auf einem KI-Modell beruht und in
der Lage ist, einer Vielzahl von Zwecken zu dienen (Art.3
Nr.66 KI-VO).

wurde, gewünschte Ergebnisse auf bestimmte Anfragen
anhand von Wahrscheinlichkeiten zu berechnen und das in
eine Vielzahl nachgelagerter KI-Systeme integriert werden
kann; bestehen Ergebnisse aus natürlicher Sprache, wird
von (KI-)Sprachmodell (engl.: Large Language Model, kurz:
LLM) gesprochen (vgl. Art.3 Nr.63 KI-VO).

KI-System: Computerprogramm, das für einen in unter-
schiedlichem Grade autonomen Betrieb ausgelegt ist, das
nach seiner Betriebsaufnahme anpassungsfähig sein kann
und das aus den erhaltenen Eingaben ableitet, wie Aus-
gaben (z. B. Vorhersagen, Inhalte, Empfehlungen oder Ent-
scheidungen) erstellt werden (Art.3 Nr.i KI-VO).

KI-VO: KI-Verordnung; Gesetz zur Regulierung künstlicher
Intelligenz.

Maschinelles Lernen: Verfahren, in dem Systeme darauf
trainiert werden, die Handlungsanweisungen zur Berech-
nung von Ergebnissen auf Grundlage von Mustern und Kor-
relationen in großen Datensätzen selbstständig zu erstellen
und anzupassen.

Natural Language Processing: Teilgebiet der KI-Forschung,
dessen Ziel es ist, die Verarbeitung natürlicher Sprache
durch Computerprogramme zu ermöglichen.

OpenAI: Unternehmen, das die KI-Sprachmodelle der
GPT-Reihe und das darauf beruhende KI-System ChatGPT
entwickelt hat.

Personenbezogene Daten: Informationen, die sich auf eine
identifizierte oder identifizierbare natürliche Person be-
ziehen (Art.4 Nr.i. DS-GVO).

Prompt: Eingabe des Nutzers in ein KI-System,
typischerweise in einen Chatbot.

Social Scoring: Bewertung oder Einstufung von Personen
über einen bestimmten Zeitraum auf der Grundlage ihres
sozialen Verhaltens, ihrer persönlichen Eigenschaften oder
ihrer Persönlichkeitsmerkmale (vgl. Art.5 Abs.1 Buchstabe
c KI-VO).

Urheber: Schöpfer eines Werkes der Literatur, Wissenschaft
oder Kunst (§7 UrhG).

GPT-4: KI-Sprachmodell der Firma OpenAl, das bestimmten
Versionen des Chatbots ChatGPT zugrunde liegt.

UrhG: Urheberrechtsgesetz; Gesetz zum Schutz der Rechte
des Urhebers an seinem Werk.

Halluzination: Erzeugung von Inhalten durch ein KI-
System, die faktisch inkorrekt sind, Informationen aus
Quellen verfälschen oder unsinnige Aussagen enthalten.

Verantwortlicher: Jeder, der über die Zwecke und Mittel
einer Verarbeitung personenbezogener Daten entscheidet
(Art.4 Nr.7 DS-GVO).

KI-Modell (Sprachmodell  /  LLM): Wahrscheinlichkeits-
modell, das mit einer großen Datenmenge darauf trainiert

Whistleblowing: Meldung von rechtlichen Verstößen in-
nerhalb eines Unternehmens oder an zuständige Behörden.

4 6  1 Glo5sar

KI-Checkliste für Betreiber

O Bestandsaufnahme

El Eingesetzte und geplante KI-Systeme

identifizieren

KI-VO
O Verbotene Systeme

Überprüfen, ob KI-Systeme folgende verbotene
Praktiken beinhalten, insbesondere:

EI Manipulation oder unterschwellige Beeinflus-

sung von Mitarbeitern

O Transparente Information der Betroffenen über

KI-Einsatz

EI Mitarbeiterschulung zum korrekten Umgang mit

KI-Systemen durchführen

O Regelmäßige Risikoabschätzungen etablieren —
auch durch Datenschutzfolgeabschätzungen

EI Kooperation mit Behörden und KI-Anbietern

sicherstellen

El Menschliche Aufsicht über KI-Entscheidungen

gewährleisten

o  Ausnutzung von Schwächen oder Schutzbe-

dürftigkeit bestimmter Gruppen

Achtung „Upgrade-Gefahr" — Anbieterwechsel nach
Art. 25 KI-VO

O Bewertung oder Einstufung der Vertrauens-

würdigkeit von Mitarbeitern

O Anbieterrolle vermeiden

o  Emotionserkennung am Arbeitsplatz (außer aus

zwingenden Sicherheitsgründen)

Falls verbotene Praktiken identifiziert wurden:

O Einstellung des Einsatzes bis spätestens
1. Dezember 2024 planen und alternative
Lösungen finden

O Hochriskante Systeme

Prüfen, ob eingesetzte oder geplante KI-Systeme in
folgende Kategorien fallen:

O Bewerberauswahl und Einstellung

O Entscheidungen über Beförderungen oder

Kündigungen

•  Aufgabenzuweisung

o  Leistungs- und Verhaltensbewertung von

Mitarbeitern

El Ausnahmen nach Art.6 Abs. 3 KI-VO

Falls ja, könnten Betreiberpflichten bei Hochrisiko-
Systemen greifen, darunter insbesondere:

EI KI-System gemäß Gebrauchsanweisung an-

wenden

o  Aufzeichnung und Überwachung des Einsatzes

und der Aktivität des KI-Systems

O Keine Zweckänderung in Richtung Hochrisiko

El KI-System nicht unter eigenem Namen/Marke

vertreiben

O Keine wesentlichen Änderungen an Hochrisiko-

KI vornehmen

EI Falls doch: Kosten-Nutzen-Abwägung: Eigen-/

Fortentwicklung als Anbieter vs. Lizenzierung in
der Betreiberstellung

Datenschutzrecht

•  Use case festlegen

EI Rechte und Pflichten in Unternehmensrichtlinie

festlegen

D Mitarbeiter schulen

O Datenschutzkonformen Einsatz prüfen (lassen)

D Dokumentation ergänzen

13 Rechtsgrundlagen prüfen

O Informationspflichten anpassen

O Gegebenenfalls Vertrag zur Auftragsverarbeitung

abschließen

El Schwellwertanalyse zur Datenschutz-Folgenab-

schätzung durchführen

O Prozess zum Umgang mit Betroffenenrechten

prüfen

KI-Checkliste für Betreiber 1  4 7

Arbeitsrecht
Richtlinien für den KI-Einsatz im Unternehmen er-
stellen

O Festlegen, wer KI-Systeme in welchem Umfang und

zu welchem Zweck nutzen darf

Urheberrecht
o  Sicherstellen, dass keine urherberrechtlich ge-
schützten Inhalte in das KI-System eingegeben
werden

D  KI-Ausgabe darauf prüfen, ob geschützte Werke

O Eingabe von personenbezogenen Daten und Ge-

darin enthalten sind; dann nicht weiterverwenden

schäftsgeheimnissen regeln

Bei KI im Bewerbungsprozess:

0  Überwachen ob das KI-System möglicherweise
diskriminierende Resultate liefert und gegebe-
nenfalls übersteuern

Betriebsrat einbeziehen:

O Mitbestimmungsrechte beachten, wenn KI-Systeme
zur Verhaltens- oder Leistungsüberwachung einge-
setzt werden

EI Betriebsrat ist zu informieren, wenn KI-Systeme

eingesetzt werden

O A u f  eigener Website den Widerspruch gegen die Nut-
zung als Trainingsdaten maschinenlesbar erklären

Jugendschutz

O Anbieter auswählen, welche Filter zur Vermeidung

jugendgefährdender Ausgaben vorsehen

Schutz von Geschäftsgeheimnissen

Vertraulichkeitsvereinbarungen mit KI-
Anbietern

0  Keine Eingabe von sensiblen Geschäftsdaten

Checkliste Prompts

1. Klare und präzise Sprache
Achten Sie darauf, dass Ihre Eingaben eindeutig und
leicht verständlich sind. Verwenden Sie eine klare und
präzise Sprache.

2. Beispiele und Vergleiche
Veranschaulichen Sie Ihre Eingaben durch Beispiele und
Vergleiche. Dies kann helfen, komplexe Sachverhalte zu
verdeutlichen und stellt sicher, dass die beabsichtigte
Bedeutung richtig verstanden wird.

3. Kontext
Definieren Sie den Kontext, damit das KI-System er-
kennen kann, für welche Zielgruppe und für welchen
Zweck Sie die Ergebnisse benötigen. Geben Sie auch an,
wann und wo Sie die Inhalte veröffentlichen wollen.

4. Fachbegriffe
Nutzen Sie spezifische Begriffe und Fachwörter, die für
das Thema relevant sind. Dies erhöht die Genauigkeit
der Antworten und stellt sicher, dass das KI-System die
richtigen Informationen bereitstellt.

5. Wiederholungen
Scheuen Sie sich nicht davor, Ihre Eingaben zu wieder-
holen und Ihre Prompts kontinuierlich anzupassen.
Selbst bei Verwendung desselben Prompts erhalten Sie
Antworten, die sich geringfügig unterschieden.

6. Nutzung von Feedback
Geben Sie dem KI-System ein Feedback, egal ob Sie mit
den Antworten zufrieden sind oder Verbesserungsbe-
darf besteht. Anhand Ihrer Feedbacks kann die KI die
Qualität der Antworten verbessern.

4 8  1 KI-Checkliste fui getietbet

Erste Hilfe zur KI-Verordnung

Ab Februar 2025 muss jeder, der KI-Systeme
wie ChatGPT in eigener Verantwortung zu
beruflichen Zwecken verwendet, nach der
KI-Verordnung eine entsprechende KI-Kom-
petenz besitzen und in seinem Unternehmen
vermitteln. Die Broschüre hilft dabei, die
neue Rechtspflicht zu erfüllen. Sie erklärt
das neue KI-Recht anschaulich, ordnet es
anhand von Beispielen ein und gibt wertvolle
Pra)dstipps auf dem Weg zur KI-Kompetenz.
Neben der KI-Verordnung werden auch
andere Rechtsgebiete wie Datenschutz-,
Urheber- und Arbeitsrecht mit einbezogen.

Behandelt werden unter anderem
folgende Fragen:

•  Wie erwirbt man KI-Kompetenz?
•  Was ist ein KI-System und was bedeutet es,

dass es autonom agiert?

•  Warum kann KI nicht denken und

trotzdem in menschlicher Sprache sinnvoll
antworten und Fragen stellen?

•  Welche Nutzung von KI-Systemen ist

gefahrlos möglich?

•  Wo muss man aufpassen?
•  Was bedeutet „prompten" und

wie geht das?

•  Wie setzt man sich mit KI-Ergebnissen

auseinander?

•  Wie behält man als Mensch die Kontrolle

über das Werkzeug KI?

•  Was bedeutet der Einsatz von KI für

das Lernen?

•  Wo kann die Technik helfen, wo nicht?
•  Welche Anforderungen gelten neben

dem KI-Recht?

Zum Autorenteam
Prof. Dr. Rolf Schwartmann ist Professor
an der Technischen Hochschule Köln und
Leiter der Kölner Forschungsstelle für Medien-
recht, Privatdozent an der Johannes Guten-
berg-Universität Mainz und Vorsitzender
der Gesellschaft für Datenschutz und Daten-
sicherheit (GDD) e.V.

Kristin Benedikt ist Richterin am Verwal-
tungsgericht und Mitglied im Vorstand der
Gesellschaft für Datenschutz und Daten-
sicherheit (GDD) e.V.

Moritz Köhler ist Mitarbeiter der Kölner
Forschungsstelle für Medienrecht an der
Technischen Hochschule Köln und Doktorand
bei Prof. Dr. Rolf Schwartmann an der
Johannes Gutenberg-Universität Mainz.

Dr. Markus Wünschelbaum ist Persönlicher
Referent für Policy und Datenstrategie des
Hamburgischen Beauftragten für Datenschutz
und Informationsfreiheit.

bedule 1 chbeck.dejnachhaitig

ISBN 978-3406-82nm

I N   H I   I M I l d

9  7 8 3 4 0 6  827181 €9 , 9 0


